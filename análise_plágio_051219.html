<meta http-equiv='Content-Type' content='text/html; charset=UTF-8'><style type="text/css"> body, table { font-size: 14px } /* Table Style - This is what you want ------------------------------------------------------------------ */ table a:link, table a:visited { 	color: #2D4FD6; 	text-decoration:none; } table a:active, table a:hover { 	color: #bd5a35; 	text-decoration:underline; }  /* para descatar os candidatos similares*/  table tr.sim_1, table tr.sim_1 a { color: #C74444; }    table { 	font-family:Arial, Helvetica, sans-serif; 	color:#333; 	/* font-size:12px; 	text-shadow: 1px 1px 0px #fff;  */ 	background:#eaebec; 	border:#ccc 1px solid;     /* 	-moz-border-radius:3px; 	-webkit-border-radius:3px; 	border-radius:3px;  	-moz-box-shadow: 0 1px 2px #d1d1d1; 	-webkit-box-shadow: 0 1px 2px #d1d1d1; 	box-shadow: 0 1px 2px #d1d1d1;         */     width: 100%; } table th { 	padding:5px 5px 5px 5px; 	border-top:1px solid #fafafa; 	border-bottom:1px solid #e0e0e0;  	background: #ededed; 	background: -webkit-gradient(linear, left top, left bottom, from(#ededed), to(#ebebeb)); 	background: -moz-linear-gradient(top,  #ededed,  #ebebeb); } table th:first-child{ 	text-align: left; 	padding-left:20px; } table tr:first-child th:first-child{ 	-moz-border-radius-topleft:3px; 	-webkit-border-top-left-radius:3px; 	border-top-left-radius:3px; } table tr:first-child th:last-child{ 	-moz-border-radius-topright:3px; 	-webkit-border-top-right-radius:3px; 	border-top-right-radius:3px; } table tr{ 	text-align: center; 	padding-left:20px; } table tr td:first-child{ 	text-align: left; 	padding-left:20px; 	border-left: 0; } table tr td { 	padding:5px 5px 5px 5px; 	/* border-top: 1px solid #ffffff; 	border-bottom:1px solid #e0e0e0; 	border-left: 1px solid #e0e0e0;     */  	background: #fafafa; 	background: -webkit-gradient(linear, left top, left bottom, from(#fbfbfb), to(#fafafa)); 	background: -moz-linear-gradient(top,  #fbfbfb,  #fafafa); } table tr.even td{ 	background: #f6f6f6; 	background: -webkit-gradient(linear, left top, left bottom, from(#f5f5f5), to(#f3f3f3)); 	background: -moz-linear-gradient(top,  #f8f8f8,  #f6f6f6); } table tr:last-child td{  border-bottom:0; } table tr:last-child td:first-child{ 	-moz-border-radius-bottomleft:3px; 	-webkit-border-bottom-left-radius:3px; 	border-bottom-left-radius:3px; } table tr:last-child td:last-child{ 	-moz-border-radius-bottomright:3px; 	-webkit-border-bottom-right-radius:3px; 	border-bottom-right-radius:3px; } table tr:hover td{ 	background: #f2f2f2; 	background: -webkit-gradient(linear, left top, left bottom, from(#E4EDFF), to(#E4EDFF)); 	background: -moz-linear-gradient(top,  #f2f2f2,  #f0f0f0); }  .common_chunk{color:#f00;text-decoration: none;}.divisor-texto{display: none;} #cand_listAll{display: block;} span.f1 {   color:#ff0000; } </style><script>function toggle( idCandidate ){ 	var candidateToShow = 'cand_'+idCandidate;	var divs = document.getElementsByClassName('divisor');	for (i = 0; i < divs.length; i++)	{		divs[i].style.display = 'none';	}	if( candidateToShow && document.getElementById(candidateToShow) ){		document.getElementById(candidateToShow).style.display = 'block';			}		if( $('.select-cand') ){	  $('.select-cand').removeClass('active');	}	if( $('#li-cand-'+idCandidate) ){	  $('#li-cand-'+idCandidate).addClass('active');	}		return false;	}</script><script>window.onload = function() {toggle("listAll");};</script>

<div id'divisor-list' class='col-xs-6 col-sm-6 col-md-4 col-lg-3'>
<ul class='nav nav-pills nav-stacked'>
<li role='presentation' id='li-cand-listAll' class='select-cand'><a href='#' onclick='return toggle("listAll");'>Documentos candidatos </a></li>
<li role='presentation' id='li-cand-_file2' class='select-cand'><a href='#' onclick='return toggle("_file2");'>devmedia.com.br/extr... [0,46%]</a></li>
<li role='presentation' id='li-cand-_file6' class='select-cand'><a href='#' onclick='return toggle("_file6");'>blog.cartorio24horas... [0,32%]</a></li>
<li role='presentation' id='li-cand-_file1' class='select-cand'><a href='#' onclick='return toggle("_file1");'>canaltech.com.br/bus... [0,27%]</a></li>
<li role='presentation' id='li-cand-_file3' class='select-cand'><a href='#' onclick='return toggle("_file3");'>pt.wikipedia.org/wik... [0,22%]</a></li>
<li role='presentation' id='li-cand-_file5' class='select-cand'><a href='#' onclick='return toggle("_file5");'>economia.uol.com.br/... [0,17%]</a></li>
<li role='presentation' id='li-cand-_file8' class='select-cand'><a href='#' onclick='return toggle("_file8");'>oecd.org/education/B... [0,15%]</a></li>
<li role='presentation' id='li-cand-_file7' class='select-cand'><a href='#' onclick='return toggle("_file7");'>agendor.com.br/blog/... [0,11%]</a></li>
<li role='presentation' id='li-cand-_file10' class='select-cand'><a href='#' onclick='return toggle("_file10");'>onigrama.com.br/10-f... [0,1%]</a></li>
<li role='presentation' id='li-cand-_file9' class='select-cand'><a href='#' onclick='return toggle("_file9");'>docs.microsoft.com/e... [0,03%]</a></li>
<li role='presentation' id='li-cand-_file4' class='select-cand'><a href='#' onclick='return toggle("_file4");'>hepg.org/her-home/is... [0,01%]</a></li>
</ul>
</div>

<div id='content-candidates' class='col-xs-6 col-sm-6 col-md-8 col-lg-9' >
<div class='divisor divisor-texto' id="cand_listAll"><span><hr class='text-separator'><br>Arquivo de entrada: <a target='_blank' href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)</span><table cellspacing='0' id='table-indice-report'><thead><tr class='table-ui-widget-header'><th>Arquivo encontrado</th><th width='10%'></th><th width='10%'>Total de termos</th><th width='10%'>Termos comuns</th><th width='10%'>Similaridade (%)</th><th width='13%'></th></tr></thead><tbody><tr class=' even'>
<td><a href='https://www.devmedia.com.br/extract-transformation-and-load-etl-ferramentas-bi/24408' title='https://www.devmedia.com.br/extract-transformation-and-load-etl-ferramentas-bi/24408' target='_blank'>devmedia.com.br/extr...</a></td>
<td><a href='#' onclick='return toggle("_file2");'>Visualizar</a></td>
<td>795</td>
<td>55</td>
<td>0,46</td>
<td></td>
</tr><tr class=''>
<td><a href='http://blog.cartorio24horas.com.br/onus-reais-no-cartorio-de-registro-de-imoveis/' title='http://blog.cartorio24horas.com.br/onus-reais-no-cartorio-de-registro-de-imoveis/' target='_blank'>blog.cartorio24horas...</a></td>
<td><a href='#' onclick='return toggle("_file6");'>Visualizar</a></td>
<td>9135</td>
<td>65</td>
<td>0,32</td>
<td></td>
</tr><tr class=' even'>
<td><a href='https://canaltech.com.br/business-intelligence/entendendo-o-processo-de-etl-22850/' title='https://canaltech.com.br/business-intelligence/entendendo-o-processo-de-etl-22850/' target='_blank'>canaltech.com.br/bus...</a></td>
<td><a href='#' onclick='return toggle("_file1");'>Visualizar</a></td>
<td>1125</td>
<td>33</td>
<td>0,27</td>
<td></td>
</tr><tr class=''>
<td><a href='https://pt.wikipedia.org/wiki/Negros' title='https://pt.wikipedia.org/wiki/Negros' target='_blank'>pt.wikipedia.org/wik...</a></td>
<td><a href='#' onclick='return toggle("_file3");'>Visualizar</a></td>
<td>2997</td>
<td>31</td>
<td>0,22</td>
<td></td>
</tr><tr class=' even'>
<td><a href='https://economia.uol.com.br/noticias/redacao/2017/04/05/entre-2043-bilionarios-so-10-sao-negros-incluindo-oprah-e-michael-jordan.htm' title='https://economia.uol.com.br/noticias/redacao/2017/04/05/entre-2043-bilionarios-so-10-sao-negros-incluindo-oprah-e-michael-jordan.htm' target='_blank'>economia.uol.com.br/...</a></td>
<td><a href='#' onclick='return toggle("_file5");'>Visualizar</a></td>
<td>2809</td>
<td>25</td>
<td>0,17</td>
<td></td>
</tr><tr class=''>
<td><a href='http://www.oecd.org/education/Brazil-country-profile.pdf' title='http://www.oecd.org/education/Brazil-country-profile.pdf' target='_blank'>oecd.org/education/B...</a></td>
<td><a href='#' onclick='return toggle("_file8");'>Visualizar</a></td>
<td>8001</td>
<td>30</td>
<td>0,15</td>
<td></td>
</tr><tr class=' even'>
<td><a href='https://www.agendor.com.br/blog/tecnicas-de-vendas-e-atendimento/' title='https://www.agendor.com.br/blog/tecnicas-de-vendas-e-atendimento/' target='_blank'>agendor.com.br/blog/...</a></td>
<td><a href='#' onclick='return toggle("_file7");'>Visualizar</a></td>
<td>1576</td>
<td>14</td>
<td>0,11</td>
<td></td>
</tr><tr class=''>
<td><a href='https://www.onigrama.com.br/10-fontes-gratis-que-tornarao-sua-apresentacao-fantastica/' title='https://www.onigrama.com.br/10-fontes-gratis-que-tornarao-sua-apresentacao-fantastica/' target='_blank'>onigrama.com.br/10-f...</a></td>
<td><a href='#' onclick='return toggle("_file10");'>Visualizar</a></td>
<td>1162</td>
<td>13</td>
<td>0,1</td>
<td></td>
</tr><tr class=' even'>
<td><a href='https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/etl' title='https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/etl' target='_blank'>docs.microsoft.com/e...</a></td>
<td><a href='#' onclick='return toggle("_file9");'>Visualizar</a></td>
<td>1134</td>
<td>4</td>
<td>0,03</td>
<td></td>
</tr><tr class=''>
<td><a href='https://www.hepg.org/her-home/issues/harvard-educational-review-volume-74-issue-4/herarticle/_38' title='https://www.hepg.org/her-home/issues/harvard-educational-review-volume-74-issue-4/herarticle/_38' target='_blank'>hepg.org/her-home/is...</a></td>
<td><a href='#' onclick='return toggle("_file4");'>Visualizar</a></td>
<td>4616</td>
<td>3</td>
<td>0,01</td>
<td></td>
</tr></tbody></table></div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file1"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='https://canaltech.com.br/business-intelligence/entendendo-o-processo-de-etl-22850/' target='_blank'>https://canaltech.com.br/business-intelligence/entendendo-o-processo-de-etl-22850/</a> (1125 termos)<br><br>Termos comuns: 33<br>Similaridade: 0,27%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>https://canaltech.com.br/business-intelligence/entendendo-o-processo-de-etl-22850/</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – <span class='f1 c_15' id='c_15_1' href='#c_15_2' cs_f='.c_15' >Extract, Transform and Load</span> (<span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >Extração, Transformação e Carga</span>)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >o processo de</span> Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido a necessidade de uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >o processo de</span> BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >dos dados na Base</span> de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (<span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >Extração, Transformação e Carga</span>) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, uma vez que utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/<span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >o processo de</span> Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar a necessidade de pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >Business Intelligence (BI</span>) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve a necessidade de simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
<span class='f1 c_26' id='c_26_1' href='#c_26_2' cs_f='.c_26' >De acordo com</span> Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de <span class='f1 c_17' id='c_17_1' href='#c_17_2' cs_f='.c_17' >Data Warehouse (DW</span>). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >para apoiar as decisões</span> gerenciais”. Como é verificado na definição de Inmon, a necessidade de um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
As informações que serão utilizadas no processo de Business Intelligence são adquiridas por meio de um processo chamado ETL (<span class='f1 c_15' id='c_15_1' href='#c_15_2' cs_f='.c_15' >Extract, Transform and Load</span>). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, em que o "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >é a extração</span> periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >Business Intelligence (BI</span>), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados a partir de diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em que não há custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >de dados que</span> entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >para a inserção</span> de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros <span class='f1 c_13' id='c_13_1' href='#c_13_2' cs_f='.c_13' >no contexto do</span> ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do Ministério da Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 <span class='f1 c_26' id='c_26_1' href='#c_26_2' cs_f='.c_26' >de acordo com</span> o MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define o conjunto de aprendizagens essenciais que todos os alunos devem desenvolver ao longo da Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 de janeiro de 1937, no Rio de Janeiro. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no Rio de Janeiro, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que <span class='f1 c_26' id='c_26_1' href='#c_26_2' cs_f='.c_26' >de acordo com</span> o INEP (2015) a partir de 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >os dados são</span> coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus <span class='f1 c_33' id='c_33_1' href='#c_33_2' cs_f='.c_33' >dados de acordo com</span> seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
<span class='f1 c_26' id='c_26_1' href='#c_26_2' cs_f='.c_26' >De acordo com</span> o Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >Os dados são</span> publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >Os dados são</span> disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >Os dados são</span> públicos para o maior público possível.<br>
Processáveis por máquina: <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >Os dados são</span> estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence <span class='f1 c_13' id='c_13_1' href='#c_13_2' cs_f='.c_13' >no contexto do</span> presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >o processo de</span> tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional <span class='f1 c_29' id='c_29_1' href='#c_29_2' cs_f='.c_29' >onde os dados são</span> gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: <span class='f1 c_17' id='c_17_1' href='#c_17_2' cs_f='.c_17' >Data Warehouse (DW</span>), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, em que a ferramenta de BI escolhida acessa <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >os dados de</span> um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que é um tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site <span class='f1 c_12' id='c_12_1' href='#c_12_2' cs_f='.c_12' >para a criação</span> de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >se faz necessária</span> uma fase intermediária antes de carregar os dados no DW, para isso tem-se <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >a Staging Area</span> ou Data Stage. Com todos os dados já na máquina é iniciada a montagem dos processos de ETL para fazer <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a carga no</span> Banco de Dados de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, em que o PDI encontra os arquivos, prepara-os <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >para a inserção</span> e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >para a inserção</span>, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >os dados são</span> apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >do Data Warehouse</span>, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar que não há uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down em que o DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >após a extração</span> de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >do Data Warehouse</span>. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas <span class='f1 c_22' id='c_22_1' href='#c_22_2' cs_f='.c_22' >de dimensões e</span> uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se a necessidade de criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >do Data Warehouse</span><br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há a necessidade de criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para <span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >cada uma delas</span> e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	No momento que todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >dos dados dos</span> códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >dos dados dos</span> códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >dos dados dos</span> municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural dos Estados Unidos: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >dos dados dos</span> dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >os dados são</span> mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >do Data Warehouse</span>, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa que ele tem nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) <span class='f1 c_16' id='c_16_1' href='#c_16_2' cs_f='.c_16' >que faz a</span> substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, mas pode ser útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e os Estados Unidos estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado de São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do Rio de Janeiro com 415 mil resultados e após o município de São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
A partir de uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo em relação à quantidade total de alunos negros, pois a partir de programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca de 84,72% da população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de <span class='f1 c_18' id='c_18_1' href='#c_18_2' cs_f='.c_18' >dados na base</span> do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, <span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >a etapa de</span> Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, <span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >a etapa de</span> Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, <span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >a etapa de</span> Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >o processo de</span> Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, a partir de visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento <span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >para a correta</span> estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >o processo de</span> Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, a partir deste estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de um processo de Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >os dados de</span> todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos de ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >do Data Warehouse</span>	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
<span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >Persistência dos dados</span>	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file2"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='https://www.devmedia.com.br/extract-transformation-and-load-etl-ferramentas-bi/24408' target='_blank'>https://www.devmedia.com.br/extract-transformation-and-load-etl-ferramentas-bi/24408</a> (795 termos)<br><br>Termos comuns: 55<br>Similaridade: 0,46%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>https://www.devmedia.com.br/extract-transformation-and-load-etl-ferramentas-bi/24408</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >Banco de Dados</span> e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >de Análise de</span> Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
<span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >ETL – Extract, Transform and Load</span> (<span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >Extração, Transformação e Carga</span>)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >Banco de Dados</span>)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
<span class='f1 c_50' id='c_50_1' href='#c_50_2' cs_f='.c_50' >Figura 8 - Visão do</span> Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >o processo de</span> Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido <span class='f1 c_44' id='c_44_1' href='#c_44_2' cs_f='.c_44' >a necessidade de</span> uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >o processo de</span> BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (<span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >Extração, Transformação e Carga</span>) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, uma vez que utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/<span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >o processo de</span> Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar <span class='f1 c_44' id='c_44_1' href='#c_44_2' cs_f='.c_44' >a necessidade de</span> pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o Business Intelligence (BI) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve <span class='f1 c_44' id='c_44_1' href='#c_44_2' cs_f='.c_44' >a necessidade de</span> simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado <span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >aos gerentes e</span> executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, <span class='f1 c_47' id='c_47_1' href='#c_47_2' cs_f='.c_47' >Data Warehouse para</span> armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
De acordo com Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento <span class='f1 c_22' id='c_22_1' href='#c_22_2' cs_f='.c_22' >dos dados de</span> formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span> multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span> relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações <span class='f1 c_38' id='c_38_1' href='#c_38_2' cs_f='.c_38' >em Tempo Real</span>) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span> é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >os dados que serão</span> usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “<span class='f1 c_26' id='c_26_1' href='#c_26_2' cs_f='.c_26' >Data Warehouse (que</span> no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, <span class='f1 c_44' id='c_44_1' href='#c_44_2' cs_f='.c_44' >a necessidade de</span> um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >Os dados que</span> estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto <span class='f1 c_17' id='c_17_1' href='#c_17_2' cs_f='.c_17' >o Data Warehouse</span> agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
<span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >As informações que serão</span> utilizadas no processo de Business Intelligence são adquiridas por meio de um processo chamado <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >ETL (Extract, Transform and Load</span>). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >Banco de Dados</span> e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, em que o "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >para que sejam</span> posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes <span class='f1 c_49' id='c_49_1' href='#c_49_2' cs_f='.c_49' >fontes de dados</span> (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "<span class='f1 c_33' id='c_33_1' href='#c_33_2' cs_f='.c_33' >carregados" no Data Warehouse</span>. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao <span class='f1 c_55' id='c_55_1' href='#c_55_2' cs_f='.c_55' >Warehouse para que</span> eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita <span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >no Data Warehouse</span> e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >de análise de</span> negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >uma ferramenta de</span> gerenciamento de Business Intelligence (BI), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados <span class='f1 c_34' id='c_34_1' href='#c_34_2' cs_f='.c_34' >a partir de</span> diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em que não há custo algum com licenças. É <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >a ferramenta de</span> código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar <span class='f1 c_49' id='c_49_1' href='#c_49_2' cs_f='.c_49' >fontes de dados</span> para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa <span class='f1 c_51' id='c_51_1' href='#c_51_2' cs_f='.c_51' >de dados que</span> entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >de análise de negócios</span> que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados <span class='f1 c_38' id='c_38_1' href='#c_38_2' cs_f='.c_38' >em tempo real</span>, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >uma ferramenta de</span> Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande <span class='f1 c_36' id='c_36_1' href='#c_36_2' cs_f='.c_36' >volume de dados</span> e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto do ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >de 1871; análise de</span> projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do Ministério da Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 de acordo com o MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define o conjunto de aprendizagens essenciais que todos os alunos devem desenvolver ao longo da Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 de janeiro de 1937, no <span class='f1 c_45' id='c_45_1' href='#c_45_2' cs_f='.c_45' >Rio de Janeiro</span>. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no <span class='f1 c_45' id='c_45_1' href='#c_45_2' cs_f='.c_45' >Rio de Janeiro</span>, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que de acordo com o INEP (2015) <span class='f1 c_34' id='c_34_1' href='#c_34_2' cs_f='.c_34' >a partir de</span> 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados de acordo com seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
De acordo com o Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >o processo de</span> tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: <span class='f1 c_49' id='c_49_1' href='#c_49_2' cs_f='.c_49' >Fontes de Dados</span> (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >de um Data</span> Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >de um Data Warehouse</span> dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: <span class='f1 c_48' id='c_48_1' href='#c_48_2' cs_f='.c_48' >Análise dos resultados</span>, <span class='f1 c_15' id='c_15_1' href='#c_15_2' cs_f='.c_15' >em que a ferramenta de</span> BI escolhida acessa os dados <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >de um Data Warehouse ou</span> <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >de um Data</span> Mart <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >para que sejam</span> feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – <span class='f1 c_49' id='c_49_1' href='#c_49_2' cs_f='.c_49' >Fontes de Dados</span> (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que é um tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. Com todos os dados já na máquina é iniciada a montagem dos processos de ETL para fazer a carga no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >Banco de Dados</span> de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, em que o PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >carga dos dados</span> estruturados em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >carga dos dados</span> estruturados em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span> Staging foi finalizado na forma da imagem abaixo:<br>
<span class='f1 c_50' id='c_50_1' href='#c_50_2' cs_f='.c_50' >Figura 8 - Visão do</span> Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >do Data Warehouse</span>, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar que não há uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down em que o DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após <span class='f1 c_12' id='c_12_1' href='#c_12_2' cs_f='.c_12' >a extração de dados</span>, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande <span class='f1 c_26' id='c_26_1' href='#c_26_2' cs_f='.c_26' >Data Warehouse que</span> é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “<span class='f1 c_17' id='c_17_1' href='#c_17_2' cs_f='.c_17' >O Data Warehouse</span> não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente <span class='f1 c_17' id='c_17_1' href='#c_17_2' cs_f='.c_17' >o Data Warehouse para</span> armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >do Data Warehouse</span>. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se <span class='f1 c_44' id='c_44_1' href='#c_44_2' cs_f='.c_44' >a necessidade de</span> criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. <span class='f1 c_16' id='c_16_1' href='#c_16_2' cs_f='.c_16' >Para isso é necessário</span> levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações <span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >no Data Warehouse</span>.<br>
4.4.5 Processo ETL para carga <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >do Data Warehouse</span><br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida <span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >no Data Warehouse</span> pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há <span class='f1 c_44' id='c_44_1' href='#c_44_2' cs_f='.c_44' >a necessidade de</span> criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito <span class='f1 c_40' id='c_40_1' href='#c_40_2' cs_f='.c_40' >de formas diferentes</span> por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para cada uma delas e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >carga dos dados</span> estruturados em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >os dados que</span> se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >carga dos dados</span> estruturados em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	No momento que todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >os dados que</span> se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >carga dos dados</span> estruturados em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão <span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >no Data Warehouse</span> (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país <span class='f1 c_53' id='c_53_1' href='#c_53_2' cs_f='.c_53' >de origem o</span> Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural dos Estados Unidos: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas <span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >no Data Warehouse</span>, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, <span class='f1 c_37' id='c_37_1' href='#c_37_2' cs_f='.c_37' >mas pode ser</span> utilizado para unir diferentes fluxos de dados ou analisar <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >os dados que</span> estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >carga dos dados</span> estruturados em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se <span class='f1 c_39' id='c_39_1' href='#c_39_2' cs_f='.c_39' >a parte de</span> transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >do Data Warehouse</span>, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, <span class='f1 c_37' id='c_37_1' href='#c_37_2' cs_f='.c_37' >mas pode ser</span> utilizado para unir diferentes fluxos de dados ou analisar <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >os dados que</span> estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >carga dos dados</span> estruturados em uma tabela no <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span>. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa que ele tem nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se <span class='f1 c_39' id='c_39_1' href='#c_39_2' cs_f='.c_39' >a parte de</span> transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >banco de dados</span> pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para <span class='f1 c_17' id='c_17_1' href='#c_17_2' cs_f='.c_17' >o Data Warehouse</span> e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, <span class='f1 c_37' id='c_37_1' href='#c_37_2' cs_f='.c_37' >mas pode ser</span> útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e os Estados Unidos estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado de São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do <span class='f1 c_45' id='c_45_1' href='#c_45_2' cs_f='.c_45' >Rio de Janeiro</span> com 415 mil resultados e após o município de São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
<span class='f1 c_34' id='c_34_1' href='#c_34_2' cs_f='.c_34' >A partir de</span> uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo em relação à quantidade total de alunos negros, pois <span class='f1 c_34' id='c_34_1' href='#c_34_2' cs_f='.c_34' >a partir de</span> programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca de 84,72% da população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >o processo de</span> Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, <span class='f1 c_34' id='c_34_1' href='#c_34_2' cs_f='.c_34' >a partir de</span> visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >e carga dos dados</span>.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >carga dos dados</span>, onde a cada erro constatado, era necessária a completa limpeza <span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >no Data Warehouse</span>, procurar em qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >parte da carga</span> teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >correção de erros</span> que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >o processo de</span> Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >a partir deste</span> estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de <span class='f1 c_29' id='c_29_1' href='#c_29_2' cs_f='.c_29' >um processo de</span> Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir os dados de todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos <span class='f1 c_43' id='c_43_1' href='#c_43_2' cs_f='.c_43' >de ETL e</span> a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
<span class='f1 c_52' id='c_52_1' href='#c_52_2' cs_f='.c_52' >Tempo de construção</span> <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >do Data Warehouse</span>	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em <span class='f1 c_47' id='c_47_1' href='#c_47_2' cs_f='.c_47' >Data Warehouse para</span> simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file3"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='https://pt.wikipedia.org/wiki/Negros' target='_blank'>https://pt.wikipedia.org/wiki/Negros</a> (2997 termos)<br><br>Termos comuns: 31<br>Similaridade: 0,22%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>https://pt.wikipedia.org/wiki/Negros</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – Extract, Transform and Load (Extração, Transformação e Carga)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – <span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >Instituto Brasileiro de Geografia e Estatística</span><br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar o processo de Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido a necessidade de uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar o processo de BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >o estado da</span> arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (Extração, Transformação e Carga) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, <span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >uma vez que</span> utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/o processo de Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >em relação a</span> sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar a necessidade de pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >do negro na</span> educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o Business Intelligence (BI) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve a necessidade de simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
<span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >De acordo com</span> Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, a necessidade de um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
As informações que serão utilizadas no processo de Business Intelligence são adquiridas por meio de um processo chamado ETL (Extract, Transform and Load). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, em que o "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de Business Intelligence (BI), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados a partir de diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em que não há custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa de dados que entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >do negro na</span> sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >do negro na</span> educação formal brasileira até a atuação <span class='f1 c_16' id='c_16_1' href='#c_16_2' cs_f='.c_16' >do movimento negro</span> nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto do ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >a população negra</span> e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do Ministério da Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >de acordo com o</span> MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define o conjunto de aprendizagens essenciais que todos os alunos devem desenvolver <span class='f1 c_12' id='c_12_1' href='#c_12_2' cs_f='.c_12' >ao longo da</span> Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. <span class='f1 c_15' id='c_15_1' href='#c_15_2' cs_f='.c_15' >A respeito de</span> sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 de janeiro de 1937, no Rio de Janeiro. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no Rio de Janeiro, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >de acordo com o</span> INEP (2015) a partir de 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >de acordo com</span> seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
<span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >De acordo com o</span> Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar o processo de tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na <span class='f1 c_22' id='c_22_1' href='#c_22_2' cs_f='.c_22' >sua forma original</span>, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, em que a ferramenta de BI escolhida acessa os dados de um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que é um tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, <span class='f1 c_13' id='c_13_1' href='#c_13_2' cs_f='.c_13' >Unidades da Federação</span> (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (<span class='f1 c_28' id='c_28_1' href='#c_28_2' cs_f='.c_28' >Instituto Brasileiro de Geografia e Estatística</span>).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >Com todos os</span> dados já na máquina é iniciada a montagem dos processos de ETL para fazer a carga no Banco de Dados de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, em que o PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo do Data Warehouse, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar que não há uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down em que o DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após a extração de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais <span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >do que uma</span> junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura do Data Warehouse. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se a necessidade de criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >negros no Brasil</span> entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga do Data Warehouse<br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há a necessidade de criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para cada uma delas e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, <span class='f1 c_26' id='c_26_1' href='#c_26_2' cs_f='.c_26' >por outro lado</span>, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	No momento que todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >dos Estados Unidos</span>: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela do Data Warehouse, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa que ele tem nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, mas pode ser útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >entre todos os</span> anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >negros no Brasil</span> entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >com a maior</span> quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >os Estados Unidos</span> estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >o estado de</span> São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >estado da Bahia</span>, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >com a maior</span> concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do Rio de Janeiro com 415 mil resultados e após o município de São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >a população negra</span> do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
A partir de uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >em relação à</span> quantidade total de alunos negros, pois a partir de programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e <span class='f1 c_18' id='c_18_1' href='#c_18_2' cs_f='.c_18' >apenas 16,04% da população</span> negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >de 84,72% da população brasileira</span> está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado <span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >com o maior</span> pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo o processo de Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, a partir de visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender o processo de Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, a partir deste estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de um processo de Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir os dados de todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >o estado da</span> arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos de ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção do Data Warehouse	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file4"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='https://www.hepg.org/her-home/issues/harvard-educational-review-volume-74-issue-4/herarticle/_38' target='_blank'>https://www.hepg.org/her-home/issues/harvard-educational-review-volume-74-issue-4/herarticle/_38</a> (4616 termos)<br><br>Termos comuns: 3<br>Similaridade: 0,01%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>https://www.hepg.org/her-home/issues/harvard-educational-review-volume-74-issue-4/herarticle/_38</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – Extract, Transform and Load (Extração, Transformação e Carga)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar o processo de Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >performance of black students in</span> Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >of black students in</span> Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >performance of black students in</span> Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido a necessidade de uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar o processo de BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (Extração, Transformação e Carga) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, uma vez que utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/o processo de Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar a necessidade de pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o Business Intelligence (BI) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve a necessidade de simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
De acordo com Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, a necessidade de um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
As informações que serão utilizadas no processo de Business Intelligence são adquiridas por meio de um processo chamado ETL (Extract, Transform and Load). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, em que o "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de Business Intelligence (BI), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados a partir de diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em que não há custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa de dados que entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto do ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do Ministério da Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 de acordo com o MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define o conjunto de aprendizagens essenciais que todos os alunos devem desenvolver ao longo da Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 de janeiro de 1937, no Rio de Janeiro. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no Rio de Janeiro, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que de acordo com o INEP (2015) a partir de 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados de acordo com seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
De acordo com o Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar o processo de tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, em que a ferramenta de BI escolhida acessa os dados de um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que é um tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. Com todos os dados já na máquina é iniciada a montagem dos processos de ETL para fazer a carga no Banco de Dados de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, em que o PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo do Data Warehouse, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar que não há uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down em que o DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após a extração de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura do Data Warehouse. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se a necessidade de criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga do Data Warehouse<br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há a necessidade de criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para cada uma delas e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	No momento que todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural dos Estados Unidos: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela do Data Warehouse, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa que ele tem nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, mas pode ser útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e os Estados Unidos estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado de São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do Rio de Janeiro com 415 mil resultados e após o município de São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
A partir de uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo em relação à quantidade total de alunos negros, pois a partir de programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca de 84,72% da população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo o processo de Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, a partir de visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender o processo de Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, a partir deste estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de um processo de Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir os dados de todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos de ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção do Data Warehouse	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file5"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='https://economia.uol.com.br/noticias/redacao/2017/04/05/entre-2043-bilionarios-so-10-sao-negros-incluindo-oprah-e-michael-jordan.htm' target='_blank'>https://economia.uol.com.br/noticias/redacao/2017/04/05/entre-2043-bilionarios-so-10-sao-negros-incluindo-oprah-e-michael-jordan.htm</a> (2809 termos)<br><br>Termos comuns: 25<br>Similaridade: 0,17%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>https://economia.uol.com.br/noticias/redacao/2017/04/05/entre-2043-bilionarios-so-10-sao-negros-incluindo-oprah-e-michael-jordan.htm</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – Extract, Transform and Load (Extração, Transformação e Carga)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >do ano de</span> 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar o processo de Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido a necessidade de uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar o processo de BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (Extração, Transformação e Carga) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, uma vez que utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/o processo de Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar a necessidade de pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o Business Intelligence (BI) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve a necessidade de simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >um dos maiores</span> aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
<span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >Na década de</span> 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
<span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >Na década de</span> 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
<span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >De acordo com</span> Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, a necessidade de um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
As informações que serão utilizadas no processo de Business Intelligence são adquiridas por meio de um processo chamado ETL (Extract, Transform and Load). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, <span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >em que o</span> "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de Business Intelligence (BI), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >a partir de</span> diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >que não há</span> custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >o crescimento de</span> negócios das empresas, uma grande massa de dados que entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto do ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >do Ministério da</span> Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >de acordo com o</span> MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define o conjunto de aprendizagens essenciais que todos os alunos devem desenvolver ao longo da Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, <span class='f1 c_17' id='c_17_1' href='#c_17_2' cs_f='.c_17' >em todo o</span> Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >de janeiro de</span> 1937, no <span class='f1 c_18' id='c_18_1' href='#c_18_2' cs_f='.c_18' >Rio de Janeiro</span>. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no <span class='f1 c_18' id='c_18_1' href='#c_18_2' cs_f='.c_18' >Rio de Janeiro</span>, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	<span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >Na década de</span> 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após <span class='f1 c_12' id='c_12_1' href='#c_12_2' cs_f='.c_12' >um período de dificuldades</span>, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >de acordo com o</span> INEP (2015) <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >a partir de</span> 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >de acordo com</span> seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
<span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >De acordo com o</span> Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar o processo de tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, em que a ferramenta de BI escolhida acessa os dados de um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que é um tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. Com todos os dados já na máquina é iniciada a montagem dos processos de ETL para fazer a carga no Banco de Dados de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, <span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >em que o</span> PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo do Data Warehouse, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >que não há</span> uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down <span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >em que o</span> DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após a extração de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura do Data Warehouse. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >Data, em que</span> ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se a necessidade de criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >o país que</span> possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros no Distrito Federal entre os anos da análise?<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros por sexo entre os anos da análise?<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros nos módulos de ensino Presencial, Semipresencial e <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >a Distância entre</span> os anos da análise?<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga do Data Warehouse<br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há a necessidade de criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para cada uma delas e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	No momento que todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural <span class='f1 c_16' id='c_16_1' href='#c_16_2' cs_f='.c_16' >dos Estados Unidos</span>: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela do Data Warehouse, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa que ele tem nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, mas pode ser útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >o país que</span> possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e <span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >os Estados Unidos</span> estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >de São Paulo</span>, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do <span class='f1 c_18' id='c_18_1' href='#c_18_2' cs_f='.c_18' >Rio de Janeiro</span> com 415 mil resultados e após o município <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >de São Paulo</span> com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
<span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >A partir de</span> uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo em relação à quantidade total de alunos negros, pois <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >a partir de</span> programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros nos módulos de ensino presencial, semipresencial e <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >a distância entre</span> os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >cerca de 84,72% da</span> população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >a quantidade de</span> alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo o processo de Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >de 7 milhões de</span> alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >a partir de</span> visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender o processo de Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, <span class='f1 c_22' id='c_22_1' href='#c_22_2' cs_f='.c_22' >a partir deste</span> estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de um processo de Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir os dados de todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos de ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção do Data Warehouse	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file6"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='http://blog.cartorio24horas.com.br/onus-reais-no-cartorio-de-registro-de-imoveis/' target='_blank'>http://blog.cartorio24horas.com.br/onus-reais-no-cartorio-de-registro-de-imoveis/</a> (9135 termos)<br><br>Termos comuns: 65<br>Similaridade: 0,32%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>http://blog.cartorio24horas.com.br/onus-reais-no-cartorio-de-registro-de-imoveis/</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >por meio do</span> estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – Extract, Transform and Load (Extração, Transformação e Carga)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >por meio do</span> estudo realizado, foi possível demonstrar <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >o processo de</span> Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido <span class='f1 c_61' id='c_61_1' href='#c_61_2' cs_f='.c_61' >a necessidade de</span> uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >para o seu</span> público alvo a utilidade de demonstrar <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >o processo de</span> BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (Extração, Transformação e Carga) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo <span class='f1 c_15' id='c_15_1' href='#c_15_2' cs_f='.c_15' >tem por finalidade</span> realizar uma pesquisa aplicada, <span class='f1 c_60' id='c_60_1' href='#c_60_2' cs_f='.c_60' >uma vez que</span> utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/<span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >o processo de</span> Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características <span class='f1 c_51' id='c_51_1' href='#c_51_2' cs_f='.c_51' >de uma determinada</span> população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar <span class='f1 c_61' id='c_61_1' href='#c_61_2' cs_f='.c_61' >a necessidade de</span> pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o Business Intelligence (BI) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve <span class='f1 c_61' id='c_61_1' href='#c_61_2' cs_f='.c_61' >a necessidade de</span> simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, <span class='f1 c_53' id='c_53_1' href='#c_53_2' cs_f='.c_53' >todas as informações</span> eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >o conceito de</span> Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, <span class='f1 c_41' id='c_41_1' href='#c_41_2' cs_f='.c_41' >o conceito de</span> ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >para que os</span> sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
<span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >De acordo com</span> Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, <span class='f1 c_61' id='c_61_1' href='#c_61_2' cs_f='.c_61' >a necessidade de</span> um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >para o seu</span> comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >para que os</span> relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
<span class='f1 c_33' id='c_33_1' href='#c_33_2' cs_f='.c_33' >As informações que</span> serão utilizadas <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >no processo de</span> Business Intelligence são adquiridas por meio de um processo chamado ETL (Extract, Transform and Load). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, <span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >em que o</span> "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em <span class='f1 c_40' id='c_40_1' href='#c_40_2' cs_f='.c_40' >algum tipo de</span> visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de Business Intelligence (BI), desenvolvido <span class='f1 c_35' id='c_35_1' href='#c_35_2' cs_f='.c_35' >para fins de</span> recolher o máximo de dados diversificados e não estruturados <span class='f1 c_36' id='c_36_1' href='#c_36_2' cs_f='.c_36' >a partir de</span> diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >que não há</span> custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa de dados que entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >os dados do</span> INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto do ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do Ministério da Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >de acordo com o</span> MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >é um documento</span> normativo da maior importância, porque define o conjunto de aprendizagens essenciais <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >que todos os</span> alunos devem desenvolver ao longo da Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 <span class='f1 c_34' id='c_34_1' href='#c_34_2' cs_f='.c_34' >de janeiro de</span> 1937, no <span class='f1 c_62' id='c_62_1' href='#c_62_2' cs_f='.c_62' >Rio de Janeiro</span>. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no <span class='f1 c_62' id='c_62_1' href='#c_62_2' cs_f='.c_62' >Rio de Janeiro, e</span> dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >de acordo com o</span> INEP (2015) <span class='f1 c_36' id='c_36_1' href='#c_36_2' cs_f='.c_36' >a partir de</span> 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >de acordo com</span> seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >os dados do</span> INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
<span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >De acordo com o</span> Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >o processo de</span> tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >os dados do</span> banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >em que a</span> ferramenta de BI escolhida acessa os dados de um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas <span class='f1 c_63' id='c_63_1' href='#c_63_2' cs_f='.c_63' >para o processo e</span> quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >no próprio site</span> do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >que é um</span> tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. Com todos os dados já na máquina é iniciada a montagem dos processos de ETL <span class='f1 c_47' id='c_47_1' href='#c_47_2' cs_f='.c_47' >para fazer a</span> carga no Banco de Dados de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, <span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >em que o</span> PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, <span class='f1 c_32' id='c_32_1' href='#c_32_2' cs_f='.c_32' >como pode ser</span> visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, <span class='f1 c_32' id='c_32_1' href='#c_32_2' cs_f='.c_32' >como pode ser</span> visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, <span class='f1 c_32' id='c_32_1' href='#c_32_2' cs_f='.c_32' >como pode ser</span> visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso <span class='f1 c_64' id='c_64_1' href='#c_64_2' cs_f='.c_64' >quer dizer que</span> ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo do Data Warehouse, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >que não há uma</span> escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down <span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >em que o</span> DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após a extração de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >pode ser utilizado</span> dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura do Data Warehouse. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se <span class='f1 c_61' id='c_61_1' href='#c_61_2' cs_f='.c_61' >a necessidade de</span> criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
<span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >Qual é a</span> diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
<span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >Qual é a</span> quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, <span class='f1 c_55' id='c_55_1' href='#c_55_2' cs_f='.c_55' >que é a tabela de</span> matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga do Data Warehouse<br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há <span class='f1 c_61' id='c_61_1' href='#c_61_2' cs_f='.c_61' >a necessidade de</span> criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais <span class='f1 c_37' id='c_37_1' href='#c_37_2' cs_f='.c_37' >que não exista</span> nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para cada uma delas e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
<span class='f1 c_17' id='c_17_1' href='#c_17_2' cs_f='.c_17' >Tendo em vista que cada uma</span> das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >com base em</span> uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >com base em</span> uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí <span class='f1 c_56' id='c_56_1' href='#c_56_2' cs_f='.c_56' >o nome da</span> dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >própria tabela de</span> distritos e na tabela de municípios (step Table input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘municipio’). <br>
	No momento que <span class='f1 c_53' id='c_53_1' href='#c_53_2' cs_f='.c_53' >todas as informações</span> são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida <span class='f1 c_52' id='c_52_1' href='#c_52_2' cs_f='.c_52' >as informações referentes</span> aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, <span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >com as informações</span> de Mesorregião sendo adquiridas (step Table input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto <span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >com as informações</span> de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-<span class='f1 c_29' id='c_29_1' href='#c_29_2' cs_f='.c_29' >se as informações</span> dos códigos das UFs brasileiras (step Table input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >com base em</span> uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >com base em</span> uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar <span class='f1 c_56' id='c_56_1' href='#c_56_2' cs_f='.c_56' >o nome da</span> coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso <span class='f1 c_53' id='c_53_1' href='#c_53_2' cs_f='.c_53' >todas as informações</span> estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, <span class='f1 c_50' id='c_50_1' href='#c_50_2' cs_f='.c_50' >por exemplo, se o</span> aluno for natural dos Estados Unidos: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, <span class='f1 c_43' id='c_43_1' href='#c_43_2' cs_f='.c_43' >mas pode ser utilizado para</span> unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >com base em</span> uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado <span class='f1 c_57' id='c_57_1' href='#c_57_2' cs_f='.c_57' >o valor de</span> procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se <span class='f1 c_44' id='c_44_1' href='#c_44_2' cs_f='.c_44' >a parte de</span> transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela do Data Warehouse, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, <span class='f1 c_43' id='c_43_1' href='#c_43_2' cs_f='.c_43' >mas pode ser utilizado para</span> unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado <span class='f1 c_57' id='c_57_1' href='#c_57_2' cs_f='.c_57' >o valor de</span> procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >com base em</span> uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >com o nome</span> ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa <span class='f1 c_42' id='c_42_1' href='#c_42_2' cs_f='.c_42' >que ele tem</span> nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se <span class='f1 c_44' id='c_44_1' href='#c_44_2' cs_f='.c_44' >a parte de</span> transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para <span class='f1 c_38' id='c_38_1' href='#c_38_2' cs_f='.c_38' >que possa ser</span> ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >não tem a</span> pretensão de cobrir todo o assunto, <span class='f1 c_43' id='c_43_1' href='#c_43_2' cs_f='.c_43' >mas pode ser</span> útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, <span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >por questões de</span> visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e os Estados Unidos estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, <span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >por questões de</span> visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado de São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, <span class='f1 c_46' id='c_46_1' href='#c_46_2' cs_f='.c_46' >e por último</span> o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (<span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >por questões de</span> visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que <span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >por questões de</span> visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é <span class='f1 c_12' id='c_12_1' href='#c_12_2' cs_f='.c_12' >o município de</span> Salvador na Bahia com 440 mil alunos, seguido pelo <span class='f1 c_48' id='c_48_1' href='#c_48_2' cs_f='.c_48' >município do Rio de Janeiro</span> com 415 mil resultados e após <span class='f1 c_12' id='c_12_1' href='#c_12_2' cs_f='.c_12' >o município de</span> São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
<span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >Qual é a</span> diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
<span class='f1 c_31' id='c_31_1' href='#c_31_2' cs_f='.c_31' >Qual é a</span> quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
<span class='f1 c_36' id='c_36_1' href='#c_36_2' cs_f='.c_36' >A partir de</span> uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo em relação à quantidade total de alunos negros, pois <span class='f1 c_36' id='c_36_1' href='#c_36_2' cs_f='.c_36' >a partir de</span> programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca de 84,72% da população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
<span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >Por questões de</span> performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (<span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >por questões de</span> visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (<span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >por questões de</span> visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (<span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >por questões de</span> visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (<span class='f1 c_65' id='c_65_1' href='#c_65_2' cs_f='.c_65' >por questões de</span> visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >o processo de</span> Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, <span class='f1 c_36' id='c_36_1' href='#c_36_2' cs_f='.c_36' >a partir de</span> visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI <span class='f1 c_54' id='c_54_1' href='#c_54_2' cs_f='.c_54' >não é simples e</span> neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >o processo de</span> Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, a partir deste estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de um processo de Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir os dados de todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos de ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção do Data Warehouse	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file7"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='https://www.agendor.com.br/blog/tecnicas-de-vendas-e-atendimento/' target='_blank'>https://www.agendor.com.br/blog/tecnicas-de-vendas-e-atendimento/</a> (1576 termos)<br><br>Termos comuns: 14<br>Similaridade: 0,11%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>https://www.agendor.com.br/blog/tecnicas-de-vendas-e-atendimento/</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – Extract, Transform and Load (Extração, Transformação e Carga)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar o processo de Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido a necessidade de uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar o processo de BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >os processos de</span> ETL (Extração, Transformação e Carga) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, uma vez que utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/o processo de Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar a necessidade de pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco <span class='f1 c_13' id='c_13_1' href='#c_13_2' cs_f='.c_13' >no assunto, na</span> qual descrevia o quão potencial o Business Intelligence (BI) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve a necessidade de simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
De acordo com Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, a necessidade de um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >que não se</span> altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
As informações que serão utilizadas no processo de Business Intelligence são adquiridas por meio de um processo chamado ETL (Extract, Transform and Load). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >em que o</span> "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de Business Intelligence (BI), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados a partir de diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >base de dados</span> para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em que não há custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa de dados que entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto do ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do Ministério da Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 de acordo com o MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define o conjunto de aprendizagens essenciais que todos os alunos devem desenvolver ao longo da Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 de janeiro de 1937, no Rio de Janeiro. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no Rio de Janeiro, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que de acordo com o INEP (2015) a partir de 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados de acordo com seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
De acordo com o Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar o processo de tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, em que a ferramenta de BI escolhida acessa <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >os dados de</span> um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que é um tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. <span class='f1 c_12' id='c_12_1' href='#c_12_2' cs_f='.c_12' >Com todos os</span> dados já na máquina é iniciada a montagem dos processos de ETL para fazer a carga no Banco <span class='f1 c_9' id='c_9_1' href='#c_9_2' cs_f='.c_9' >de Dados de</span> Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >os processos de</span> ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >em que o</span> PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo do Data Warehouse, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar que não há uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >em que o</span> DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após a extração de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura do Data Warehouse. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se a necessidade de criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >uma plataforma de</span> BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga do Data Warehouse<br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há a necessidade de criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >cada uma delas</span> e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >que não se</span> relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	No momento que todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >que não se</span> relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural dos Estados Unidos: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >o valor de</span> procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela do Data Warehouse, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >o valor de</span> procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >que ele tem</span> nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, mas pode ser útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e os Estados Unidos estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado de São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do Rio de Janeiro com 415 mil resultados e após o município de São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
A partir de uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo <span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >em relação à</span> quantidade total de alunos negros, pois a partir de programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca de 84,72% da população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo o processo de Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, a partir de visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender o processo de Business Intelligence, bem como funcionalidades e características; <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >os processos de</span> ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, a partir deste estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >um processo de</span> Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >os dados de</span> todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >os processos de</span> ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção do Data Warehouse	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file8"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='http://www.oecd.org/education/Brazil-country-profile.pdf' target='_blank'>http://www.oecd.org/education/Brazil-country-profile.pdf</a> (8001 termos)<br><br>Termos comuns: 30<br>Similaridade: 0,15%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>http://www.oecd.org/education/Brazil-country-profile.pdf</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – Extract, Transform and Load (Extração, Transformação e Carga)<br>
SQL – Structured Query Language<br>
MEC – <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >Ministério da Educação</span><br>
IBM – International Business Machines Corporation<br>
<span class='f1 c_21' id='c_21_1' href='#c_21_2' cs_f='.c_21' >INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira</span><br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information <span class='f1 c_14' id='c_14_1' href='#c_14_2' cs_f='.c_14' >System (Sistema de</span> Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >da educação básica</span> brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar o processo de Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, <span class='f1 c_24' id='c_24_1' href='#c_24_2' cs_f='.c_24' >as well as a</span> brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from <span class='f1 c_22' id='c_22_1' href='#c_22_2' cs_f='.c_22' >the Brazilian government</span> open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian <span class='f1 c_16' id='c_16_1' href='#c_16_2' cs_f='.c_16' >basic education. It is</span> significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido a necessidade de uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar o processo de BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (Extração, Transformação e Carga) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, uma vez que utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/o processo de Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar a necessidade de pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o Business Intelligence (BI) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve a necessidade de simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos <span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >de nível superior</span>. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
De acordo com Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, a necessidade de um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
As informações que serão utilizadas no processo de Business Intelligence são adquiridas por meio de um processo chamado ETL (Extract, Transform and Load). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, em que o "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de Business Intelligence (BI), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados a partir de diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em que não há custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa de dados que entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >da educação básica</span> e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >do ensino médio</span> brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >de Jovens e Adultos</span> (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >Ministério da Educação</span>, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades <span class='f1 c_25' id='c_25_1' href='#c_25_2' cs_f='.c_25' >de Jovens e Adultos</span>, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >da educação básica</span> brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >Ministério da Educação</span> é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o <span class='f1 c_30' id='c_30_1' href='#c_30_2' cs_f='.c_30' >Ministério da Educação</span> e Cultura.<br>
Até 1960 de acordo com o MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >Lei de Diretrizes e Bases da Educação</span> (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define o conjunto de aprendizagens essenciais que todos os alunos devem desenvolver ao longo <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >da Educação Básica</span> e <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >do Ensino Médio</span>, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP</span>) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >Instituto Nacional de</span> Pedagogia, o Inep foi criado, por lei, em 13 de janeiro de 1937, no <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >Rio de Janeiro</span>. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para <span class='f1 c_20' id='c_20_1' href='#c_20_2' cs_f='.c_20' >Instituto Nacional de Estudos</span> Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >Rio de Janeiro</span>, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que de acordo com o INEP (2015) a partir de 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados de acordo com seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://<span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >inep.gov.br</span>/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
De acordo com o Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar o processo de tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, em que a ferramenta de BI escolhida acessa os dados de um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://<span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >inep.gov.br</span>/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que é um tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. Com todos os dados já na máquina é iniciada a montagem dos processos de ETL para fazer a carga no Banco de Dados de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, em que o PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo do Data Warehouse, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar que não há uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down em que o DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após a extração de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura do Data Warehouse. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se a necessidade de criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga do Data Warehouse<br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há a necessidade de criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para cada uma delas e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	No momento que todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural dos Estados Unidos: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela do Data Warehouse, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa que ele tem nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, mas pode ser útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 <span class='f1 c_19' id='c_19_1' href='#c_19_2' cs_f='.c_19' >da Educação Básica</span> dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e os Estados Unidos estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado de São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do <span class='f1 c_23' id='c_23_1' href='#c_23_2' cs_f='.c_23' >Rio de Janeiro</span> com 415 mil resultados e após o município de São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >de Educação do</span> Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
A partir de uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo em relação à quantidade total de alunos negros, pois a partir de programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca de 84,72% da população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo o processo de Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, a partir de visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender o processo de Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, a partir deste estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de um processo de Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir os dados de todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos de ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção do Data Warehouse	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes <span class='f1 c_27' id='c_27_1' href='#c_27_2' cs_f='.c_27' >de desenvolvimento do</span> projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file9"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/etl' target='_blank'>https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/etl</a> (1134 termos)<br><br>Termos comuns: 4<br>Similaridade: 0,03%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>https://docs.microsoft.com/en-us/azure/architecture/data-guide/relational-data/etl</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >Extract, Transform and Load</span> (Extração, Transformação e Carga)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >Online Analytical Processing</span> (Processamento Analítico Online)<br>
OLTP – <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >Online Transaction Processing</span> (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar o processo de Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido a necessidade de uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar o processo de BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (Extração, Transformação e Carga) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, uma vez que utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se que ela é classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/o processo de Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar a necessidade de pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o Business Intelligence (BI) seria com o uso da tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve a necessidade de simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (<span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >Online Analytical Processing</span>) e OLTP (<span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >Online Transaction Processing</span>), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
De acordo com Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário o uso da ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (<span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >Online Transaction Processing</span> ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, a necessidade de um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
As informações que serão utilizadas no processo de Business Intelligence são adquiridas por meio de um processo chamado ETL (<span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >Extract, Transform and Load</span>). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, em que o "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de Business Intelligence (BI), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados a partir de diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em que não há custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa de dados que entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto do ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do Ministério da Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 de acordo com o MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define o conjunto de aprendizagens essenciais que todos os alunos devem desenvolver ao longo da Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 de janeiro de 1937, no Rio de Janeiro. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no Rio de Janeiro, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que de acordo com o INEP (2015) a partir de 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados de acordo com seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
De acordo com o Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é um conjunto de conceitos e métodos para melhorar o processo de tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, em que a ferramenta de BI escolhida acessa os dados de um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que é um tipo de arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. Com todos os dados já na máquina é iniciada a montagem dos processos de ETL para fazer a carga no Banco de Dados de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é um conjunto de outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, em que o PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo do Data Warehouse, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar que não há uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down em que o DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após a extração de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura do Data Warehouse. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se a necessidade de criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga do Data Warehouse<br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há a necessidade de criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para cada uma delas e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela com um conjunto constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	No momento que todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), além de ser possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, que pode ser executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural dos Estados Unidos: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela é considerada uma escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela do Data Warehouse, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos por meio de um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou por meio de uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, que pode ser renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de um conjunto de dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa que ele tem nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, mas pode ser útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e os Estados Unidos estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado de São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do Rio de Janeiro com 415 mil resultados e após o município de São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
A partir de uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo em relação à quantidade total de alunos negros, pois a partir de programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca de 84,72% da população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo o processo de Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, a partir de visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender o processo de Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, a partir deste estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de um processo de Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir os dados de todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos de ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção do Data Warehouse	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

<div class='divisor divisor-texto' id="cand__file10"><hr class='text-separator'><br>Arquivo de entrada: <a href='#'>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</a> (11112 termos)<br>Arquivo encontrado: <a href='https://www.onigrama.com.br/10-fontes-gratis-que-tornarao-sua-apresentacao-fantastica/' target='_blank'>https://www.onigrama.com.br/10-fontes-gratis-que-tornarao-sua-apresentacao-fantastica/</a> (1162 termos)<br><br>Termos comuns: 13<br>Similaridade: 0,1%<br><br><div class='textInfo'>O texto abaixo é o conteúdo do documento <br>&nbsp;"<b>TCC - Análise do Panorama do Aluno Negro Usando BI.docx</b>". <br>Os termos em vermelho foram encontrados no documento <br>&nbsp;"<b>https://www.onigrama.com.br/10-fontes-gratis-que-tornarao-sua-apresentacao-fantastica/</b>".<br><hr class='text-separator'></div>  UNIVERSIDADE PAULISTA – UNIP<br>
INSTITUTO DE CIÊNCIAS EXATAS E TECNOLOGIA - ICET<br>
Ciência da Computação<br>
<br>
<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
<br>
<br>
<br>
<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
bRASÍLIA - DF<br>
2019<br>
DANIEL GADS MELO SOUSA<br>
Gabriel de brito silva<br>
marcelo antônio da silva júnior<br>
pedro henrique pereira de oliveira<br>
willian de sousa rodrigues<br>
<br>
ESTUDO DE CASO: Análise do Panorama da Atuação do ALUNO Negro na Educação Básica Brasileira de 2015 a 2018 utilizando BUSINESS INTELLIGENCE<br>
<br>
Trabalho de Conclusão de Curso para obtenção do título de Bacharel em Ciência da Computação da Universidade Paulista – UNIP, campus Brasília.<br>
Aprovado em: <br>
<br>
BANCA EXAMINADORA<br>
<br>
<br>
__________________________________________________<br>
Prof. Claudio Bernardo<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Josyane Lannes<br>
Universidade Paulista<br>
<br>
__________________________________________________<br>
Prof. Msc. Luiz Antônio<br>
Universidade Paulista<br>
<br>
<br>
AGRADECIMENTOS<br>
	Agradeço primeiramente a Deus pela a vida e por todas as graças a mim concedidas.<br>
	À instituição Universidade Paulista – UNIP na pessoa da coordenadora Liliane Cordeiro por ter oferecido todas as oportunidades de fomentação do conhecimento para o curso de Ciência da Computação.<br>
	Ao orientador Claudio Bernardo pela paciência, incentivo e orientação no presente trabalho.<br>
	A professora Josyane Lannes por todo o auxílio na parte de Banco de Dados e pela sua incrível e inspiradora didática nas aulas da respectiva matéria.<br>
	A Caixa e o FNDE por terem auxiliado no início da minha caminhada no mundo profissional, por meio do estágio. Em especial agradeço aos meus colegas Murillo Higor Fernandes Carvalhes e Débora Arnaud Lima Formiga pelos seus inestimáveis auxílios e incentivos referentes à área de Business Intelligence. Além da EPROJ, setor que me acolheu tão bem nos meus últimos momentos de estágio e que proporcionou a minha atuação em um setor de Análise de Dados.<br>
	Aos meus colegas de trabalho que tanto auxiliaram para a conclusão desse projeto.<br>
	Aos meus colegas de curso em que juntos compartilhamos conhecimento, dificuldades e momentos de alegria.<br>
Daniel Gads Melo Sousa<br>
<br>
<br>
<br>
LISTA DE ABREVIATURAS E SIGLAS<br>
BI – Business Intelligence (Inteligência de Negócio)<br>
DW – Data Warehouse <br>
DM – Data Mart <br>
ETL – Extract, Transform and Load (Extração, Transformação e Carga)<br>
SQL – Structured Query Language<br>
MEC – Ministério da Educação<br>
IBM – International Business Machines Corporation<br>
INEP – Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira<br>
IBGE – Instituto Brasileiro de Geografia e Estatística<br>
OLAP – Online Analytical Processing (Processamento Analítico Online)<br>
OLTP – Online Transaction Processing (Processamento de Transações Online)<br>
PDI – Pentaho Data Integration<br>
DBMS – Database Management Systems (Sistema Gerenciador de Banco de Dados)<br>
EIS – Executive Information System (Sistema de Informação Executiva)<br>
ATM – Automatic Teller Machine (Máquina de Caixa Automático)<br>
ERP – Enterprise Resource Planning<br>
CSV – Comma-separated Values<br>
UF – Unidade da Federação<br>
XLSX – Excel Microsoft Office Open XML Format Spreadsheet File<br>
<br>
<br>
<br>
<br>
LISTA DE FIGURAS<br>
Figura 1 – Hans Peter Luhn	16<br>
Figura 2 - Arquitetura do ambiente de BI	28<br>
Figura 3 - Tabela de códigos dos países	29<br>
Figura 4 - Exemplo de Transformation e Job	30<br>
Figura 5 - Visão da ETL das bases principais	31<br>
Figura 6 - Visão geral da ETL de auxiliares	31<br>
Figura 7 - Visão geral da ETL Staging	32<br>
Figura 8 - Visão do Banco Staging	33<br>
Figura 9 - Modelo Inmon	35<br>
Figura 10 - Modelo Kimball	36<br>
Figura 11 - Exemplo de modelo Estrela	37<br>
Figura 12 - Exemplo de modelo Floco de Neve	38<br>
Figura 13 - Visão geral da ETL Ano	41<br>
Figura 14 - Diagrama da ETL Localidade Distrito	43<br>
Figura 15 - Diagrama da ETL Localidade Município	47<br>
Figura 16 - Diagrama da ETL Escola	50<br>
Figura 17 - Diagrama da ETL Aluno	53<br>
Figura 18 - Contagem de cor/raça por ano	57<br>
Figura 19 - Contagem de alunos negros por ano	58<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano	59<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)	59<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)	60<br>
Figura 23 - Contagem de alunos negros por região	61<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)	61<br>
Figura 25 - Contagem de alunos negros por município (Top 10)	62<br>
Figura 26 - Contagem de alunos negros no DF por ano	63<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano	64<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano	65<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano	66<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano	67<br>
Figura 31 - Contagem de alunos por sexo por ano	68<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano	69<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano	69<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano	70<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano	71<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)	72<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)	74<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)	76<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)	78<br>
<br>
<br>
<br>
RESUMO<br>
Este estudo teve como objetivo analisar o modelo de implantação do Business Intelligence e de que forma a aplicação do BI pode contribuir com informações relevantes para o panorama da atuação do aluno negro na educação básica brasileira. Para tanto, explicou-se conceitos, técnicas e características essenciais do Business Intelligence, além de uma rápida passagem sobre o contexto educacional brasileiro básico. Os dados utilizados para a análise são os micro dados do censo escolar da educação básica brasileira do ano de 2015 a 2018, que foram coletados do site de dados abertos do governo brasileiro. A partir da análise desses dados percebeu-se como a localização, a imigração e a falta de qualidade de vida básica do ser humano influenciam no panorama do aluno negro na educação básica brasileira. É importante ressaltar que esse estudo é voltado para o conceito, implantação e utilização do Business Intelligence e como a utilização do próprio pode contribuir com informações relevantes. Enfim, por meio do estudo realizado, foi possível demonstrar o processo de Business Intelligence para analisar dados importantes sobre a atuação do aluno negro na educação básica brasileira.<br>
<br>
<br>
ABSTRACT<br>
This study aimed to analyze the implementation model of Business Intelligence and how the application of BI can provide relevant information to the panorama of the performance of black students in Brazilian basic education. To this end, we explained the concepts, techniques and essential characteristics of Business Intelligence, as well as a brief passage on the basic Brazilian educational context. The information consumed for the analysis are the microdata of the Brazilian basic education school census from 2015 to 2018, which were collected from the Brazilian government open data site. From the analysis of these data, it was observed how the place, immigration and lack of fundamental quality of life of human beings influence the panorama of black students in Brazilian basic education. It is significant to highlight that this study is focused on the concept, implementation and use of Business Intelligence and how the usage of own can give with relevant information. Finally, through the study, it was possible to indicate the Business Intelligence process to analyze significant data about the performance of black students in Brazilian primary education.<br>
<br>
<br>
1 INTRODUÇÃO<br>
	Nessa seção será apresentado o trabalho junto dos seus objetivos gerais e específicos.<br>
1.1 Justificativa<br>
Devido a necessidade de uma análise do panorama da atuação do aluno negro na educação básica brasileira essa pesquisa se justifica através da aplicação do (a) processo de Business Intelligence em contribuição para o seu público alvo a utilidade de demonstrar o processo de BI para análise de dados. <br>
1.2 Problematização<br>
Portanto, buscou-se reunir dados/informações com o propósito de responder ao seguinte problema de pesquisa: de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira? <br>
1.3 Delimitação do tema<br>
Este projeto de pesquisa delimitou-se em colher informações sobre de que forma a aplicação do processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira tendo como referência a/o Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
1.4 Objetivos<br>
O objetivo geral do trabalho e os objetivos específicos em prol da conclusão do mesmo são descritos abaixo. <br>
1.4.1 Objetivos gerais<br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do processo de Business Intelligence auxilia uma análise dos dados da atuação do aluno negro na educação básica brasileira no contexto educacional básico brasileiro, com a finalidade de comprovar a utilidade do processo de BI para análise dos dados na Base de Micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP. <br>
1.4.2 Objetivos específicos<br>
Levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos;<br>
Apresentar os indicadores sobre a atuação do aluno negro na educação brasileira e seus requisitos;<br>
Aplicar a metodologia de Business Intelligence, bem como os processos de ETL (Extração, Transformação e Carga) e a montagem do ambiente de Data Warehouse no case estudado;<br>
Desenvolver os resultados das análises através da solução de BI;<br>
1.5 Metodologia<br>
Esse estudo tem por finalidade realizar uma pesquisa aplicada, uma vez que utilizará conhecimento da pesquisa básica para resolver problemas. <br>
Para um melhor tratamento dos objetivos e melhor apreciação desta pesquisa, observou-se <span class='f1 c_10' id='c_10_1' href='#c_10_2' cs_f='.c_10' >que ela é</span> classificada como pesquisa descritiva. Detectou-se também a necessidade da pesquisa bibliográfica no momento em que se fez uso de materiais já elaborados: livros, artigos científicos, revistas, documentos eletrônicos e enciclopédias na busca e alocação de conhecimento sobre a/o processo de Business Intelligence para a (s) análise dos dados no contexto educacional básico brasileiro, correlacionando tal conhecimento com abordagens já trabalhadas por outros autores. <br>
A pesquisa assume como estudo de caso, sendo descritiva, por sua vez, expor as características de uma determinada população ou fenômeno, demandando técnicas padronizadas de coleta de dados como questionários e etc... Descreve uma experiência, uma situação, um fenômeno ou processo nos mínimos detalhes. Por exemplo, quais as características de um determinado grupo em relação a sexo, faixa etária, renda familiar, nível de escolaridade etc. Faz uso de gráficos para visualização analítica dos dados. <br>
Como procedimentos, pode-se citar a necessidade de pesquisa Bibliográfica, isso porque será feito uso de material já publicado, constituído principalmente de livros, também se entende como um procedimento importante o estudo de caso como procedimento técnico. Tem-se como base para o resultado da pesquisa um caso em específico que poderá ser expandido futuramente. <br>
A abordagem do tratamento da coleta de dados do/a estudo de caso será quantitativa, pois requer o uso de recursos e técnicas de estatística, procurando traduzir em números os conhecimentos gerados pelo pesquisador. Existirão Gráficos; obtém opinião dos entrevistados com questões fechadas, por exemplo: Qual sua área de atuação? (Saúde, Administração) Medir através de números. Por exemplo: Pesquisa de satisfação. 40% Insatisfeito, 10% Não opinaram, 50% Satisfeitos. <br>
O problema foi direcionando a pesquisa para as áreas de uma análise do panorama da atuação do aluno negro na educação básica brasileira e ainda a pesquisa como estudo de caso, sendo este com a aplicação do (a) processo de Business Intelligence uma análise geral para a (s) análise dos dados no contexto educacional básico brasileiro. Em que será feito ao indicar os indicadores relevantes sobre a atuação do negro na educação brasileira afirmando que Objetivo Geral: <br>
O presente trabalho tem como objetivo geral apresentar de que forma a aplicação do (a) processo de Business Intelligence auxilia uma análise do panorama da atuação do aluno negro na educação básica brasileira para a (s) análise dos dados no contexto educacional, com a finalidade de analisar a utilidade do processo de BI para análise de dados na/o Base de micro dados do Censo Escolar dos Anos de 2015 a 2018 do INEP.<br>
<br>
<br>
<br>
2 EMBASAMENTO TEÓRICO<br>
Nessa seção serão apresentadas as fontes, trabalhos, artigos e autores utilizados para o embasamento desse trabalho. <br>
2.1 Business Intelligence<br>
Richard Millar Devens, em 1865, foi quem introduziu o termo “Business Intelligence" (Inteligência de Negócios) em seu livro Cyclopædia of Commercial and Business Anecdotes. Lá ele conta sobre um bancário, Sir Henry Furnese, que conseguia atuar antes da competição reunindo informações e conseguindo lucrar com elas (DEVENS, 1865).<br>
Em 1958, Hans Peter, um cientista da computação da IBM, publicou um artigo que foi um marco no assunto, na qual descrevia o quão potencial o Business Intelligence (BI) seria com <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >o uso da</span> tecnologia. O artigo intitulado “A Business Intelligence System” descrevia: <br>
An automatic system is being developed to disseminate information to the various sections of any industrial, scientific or government organization. This intelligence system will utilize data-processing machines for auto-abstracting and auto-encoding of documents […] (LUHN, 1958, p. 314). <br>
Em tradução livre: “Um sistema automático está sendo desenvolvido para disseminar informação para os setores industriais, científicos ou organizações governamentais. Esse sistema de inteligência vai utilizar máquinas de processamento de dados para abstrair e codificar automaticamente os documentos”.<br>
 Após a Segunda Guerra Mundial, houve a necessidade de simplificar e organizar o grande e rápido crescimento dos dados tecnológicos e científicos. Hoje, Luhn (Figura 1) é popularmente conhecido como o pai do Business Intelligence. <br>
<br>
<br>
<br>
<br>
<br>
Figura 1 – Hans Peter Luhn<br>
<br>
Fonte: (IEEE Spectrum, 2018) <br>
A partir dos anos 60, houve o surgimento de novas formas de armazenamento como os DBMS (Database Management Systems), tendo uma evolução no modo de gerenciar grandes volumes de dados, no final da década de 70, nasce o modelo relacional no DBMS. A partir desse momento, todas as informações eram apresentadas e armazenadas em formato digital, fazendo com que fosse possível a concretização do Business Intelligence nas próximas décadas. <br>
Também nessa mesma época, os CPD (Centros de Processamento de Dados), estavam se consolidando, se transformando no meio-termo da tecnologia da informação com os negócios. Os CPD eram focados totalmente em dados, diferente da TI que o centro focava em software, hardware e redes. <br>
Com o surgimento do conceito de sistemas de informações executivas (EIS) há uma grande disseminação do assunto, sendo um dos maiores aliados aos sistemas de BI. Segundo Turban (2009, p. 27), "Esse conceito expandiu o suporte computadorizado aos gerentes e executivos de nível superior. Alguns dos recursos introduzidos foram sistemas de geração de relatórios dinâmicos multidimensionais [...]”. <br>
Na década de 80, alguns fabricantes de softwares voltados ao campo do BI começaram a ganhar terreno. Softwares como MicroStrategy, Business Objects e Crystal Reports começaram a ser populares nas empresas que começaram a usar realmente computadores na época. <br>
Em 1988 houve outro marco importante, com o intuito de simplificar as análises em BI a conferência internacional em Roma, organizada pelo Multiway Data Analysis Consortium, dá início à era moderna do Business Intelligence, sendo o termo popularizado pelo analista do Gartner Group, Howard Dresner, em 1989. <br>
Na década de 90, se populariza o conceito de Data Warehouse, como um sistema dedicado a auxiliar o BI, também separando em momentos distintos os processamentos OLAP (Online Analytical Processing) e OLTP (Online Transaction Processing), sendo o OLAP usado ao lado do BI para a montagem de relatórios e posteriormente painéis em inúmeras visões diferentes, e o OLTP sendo utilizado geralmente para explorações estatísticas. Ao mesmo tempo, por consequência, o conceito de ETL (Extraction, Transformation and Loading) é incorporado ao Data Warehouse, com o intuito de fornecer dados relevantes e fornecer uma extração focada. <br>
Os sistemas ERP (Enterprise Resource Planning) é um software voltado para a gestão das empresas, garantindo a entrada de dados essencial para que os sistemas de BI, sendo possível reportar aos gestores análises em pontos específicos. Consolidados todos os sistemas envolvidos no BI, quais sejam, Sistemas de Informações Executivas (EIS), ERP, Data Warehouse para armazenamento, OLTP, ETL e OLAP nasce o Business Intelligence 1.0 (KUMAR, 2017). <br>
É importante ter em mente sobre a construção e organização do BI é que, este processo é sem fim, sempre haverá novos requisitos a serem cumpridos mesmo tendo terminado o trabalho, sendo necessário refazer todas as etapas novamente. <br>
<br>
2.2 Sistemas de Informação OLAP/OLTP<br>
De acordo com Primak (2008), o objetivo de uma ferramenta OLAP é permitir a análise multidimensional dinâmica dos dados, apoiando os usuários finais em suas atividades, oferecendo várias perspectivas, onde o próprio usuário cria suas consultas dependendo de suas necessidades, fazendo cruzamento dos dados de formas diferenciadas, auxiliando na busca pelas respostas desejadas. <br>
Para oferecer as várias perspectivas o método mais comumente usados é o MOLAP onde se usa um banco de dados multidimensional com tabelas que mais parecem um cubo, que por esse motivo originou a denominação “dados cúbicos”. Com essas tabelas de múltiplas dimensões é possível cruzar informações que antes não seria possível por uma pessoa, fazendo assim necessário <span class='f1 c_4' id='c_4_1' href='#c_4_2' cs_f='.c_4' >o uso da</span> ferramenta. Existem também outros métodos como o ROLAP que utiliza um banco de dados relacional para seus dados e possui um tempo maior para resposta. <br>
Para Thomsen (2002) o OLAP precisa dos requisitos abaixo para funcionar: <br>
Uma estrutura dimensional; <br>
Especificação eficiente das dimensões e cálculos; <br>
Separação da estrutura e representação; <br>
Flexibilidade; <br>
Velocidade suficiente para suportar as análises ad hoc; <br>
Suporte para multiusuários; <br>
Segundo Prasad (2007) o processo OLAP se diferencia do OLTP (Online Transaction Processing ou Processamento de Transações em Tempo Real) que foca em processar transações repetitivas em alta quantidade e manipulação simples. Já o OLAP envolve uma análise de vários itens de dados em relacionamentos complexos, que busca padrões, tendências e exceções. <br>
O foco principal do OLTP é transações online. Como o nome já diz, suas consultas são simples e curtas, portanto não precisa de tanto tempo de processamento, também utiliza pouco espaço. O banco de dados é atualizado frequentemente, pode acontecer no momento da transação e também é normalizado. Um exemplo muito utilizado ao se explicar o OLTP é o ATM (do inglês, caixa automático) onde cada transação modifica a conta do usuário. <br>
2.3 Data Warehouse<br>
    As aplicações de Business Intelligence necessitam de um repositório específico para buscar os dados que serão usados para a operação, sejam eles de que tipo for. O local onde serão centralizadas essas informações é chamado de Data Warehouse (DW). Segundo Inmon (2005, p. 29) “Data Warehouse (que no português significa literalmente armazém de dados) é um deposito de dados orientado por assunto, integrado, não volátil, variável com o tempo, para apoiar as decisões gerenciais”. Como é verificado na definição de Inmon, a necessidade de um repositório específico para as informações é definida pelos seguintes itens: <br>
Orientado por assunto: Significa que os dados ali presentes têm contexto direto com as atividades da empresa, ou seja, as informações contidas são, unicamente, as necessárias para definir as operações.<br>
Integrado: Todas as atividades do DW devem estar conectadas ao ambiente operacional.<br>
Não volátil: Os dados que estão especificadamente no Warehouse não podem sofrer mudanças, tal como alterações e inclusões (já que é necessária uma informação concreta que não se altere para tomar decisões), podendo ser apenas consultados ou excluídos.<br>
Variável com o Tempo: Está ligado ao conceito da Não-Volatilidade, mas aqui com um foco maior no tempo dessas informações. Já que os dados dentro do DW não podem ser alterados, o horário das informações também não pode mudar. Por isso é necessária a certeza do tempo em que elas estão armazenadas.<br>
Enquanto o Data Warehouse agrega todos os dados, definições e relacionamentos entre eles, o Data Mart (DM) é um pequeno DW dentro do contexto maior que se preocupa em adquirir apenas uma parte dessas informações para uma operação específica. Pode ser feita uma analogia com um comerciante que possui uma loja e um armazém, ao comprar novos produtos para o seu comércio, ele, primeiramente, vai armazenar tudo o que for possível nesse armazém, para, posteriormente, pegar certas quantidades de determinados produtos e apresentá-los na sua loja para vendê-los. Isso é feito para que os relatórios gerados utilizem uma única base para adquirir as informações. <br>
2.4 O Método ETL<br>
As informações que serão utilizadas no processo de Business Intelligence são adquiridas <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >por meio de</span> um processo chamado ETL (Extract, Transform and Load). Esse processo tem como função "transportar" e transformar os dados para todas as instâncias do BI, seja na aquisição dos dados das fontes, inserção desses dados no Banco de Dados e transformação dos dados nos tipos necessários para a realização da análise. Esse processo já foi conhecido como ETLM, em que o "M" significava Maintenance (Manutenção), mas esse último já caiu em desuso (BRAGHITTONI, 2017). Cada uma das suas ações será explicada adiante. <br>
E -  Extract (Extração): A primeira parte do processo de transporte é a extração periódica das informações para que sejam posteriormente carregadas. Elas podem ser extraídas de diversas fontes, sejam arquivos do tipo CSV, tipo texto (TXT), arquivos  mainframe, sites e até arquivos em diferentes fontes de dados (CARVALHAES e ALVES, 2015). A extração deve estar preparada para reconhecer esses dados nas mais variadas fontes possíveis. <br>
T -  Transform (Transformação): Após os dados extraídos, eles precisam passar por uma verificação para depois serem "carregados" no Data Warehouse. Como Inmon (2005, p. 29) afirma que os dados dentro do DW não podem ser modificados (propriedade Não Volátil), é recomendado o uso de uma instância intermediária ao Warehouse para que eles sejam transformados segundo as regras de negócio. Essa instância pode ser outro BD chamado Staging Area ou a própria memória do servidor/computador (CARVALHAES e ALVES, 2015). <br>
L -  Load (Carga): O último processo é da carga propriamente dita no Data Warehouse e nos seus Data Marts. No final desse processo os dados já estão prontos para o uso de alguma ferramenta de BI, transformando eles em algum tipo de visualização gráfica (Dashboards). Essa carga é feita de forma incremental, já que, como mencionado anteriormente por Inmon, esses dados não podem sofrer mudanças (BRAGHITTONI, 2017). <br>
2.5 Ferramentas<br>
	Para o desenvolvimento desse trabalho foram utilizadas algumas ferramentas que serão explicadas adiante.<br>
2.5.1 Pentaho Data Integrator<br>
Com o desejo de alcançar uma mudança positiva no mercado de análise de negócio dominada por grandes vendedores que oferecem produtos baseados em plataformas com custo elevado, foi-se criado o Pentaho. É uma ferramenta de gerenciamento de Business Intelligence (BI), desenvolvido para fins de recolher o máximo de dados diversificados e não estruturados a partir de diversas fontes e analisá-los para encontrar novos padrões, indicadores de tendências e base de dados para inovação. <br>
Por ser uma tecnologia Open Source, o Pentaho possui uma versão funcional extremamente potente em que não há custo algum com licenças. É a ferramenta de código aberto mais utilizada do mundo, contendo um ambiente de desenvolvimento integrado e bastante poderoso. <br>
O Pentaho possui diversas funções a fim de entender e analisar o negócio empresarial. Algumas delas permitem que o usuário possa acessar e preparar fontes de dados para análise, mineração e geração de relatórios on-line, via web. Também vem integrado com um ambiente de desenvolvimento, baseado no Eclipse (API), que visa à resolução de soluções mais complexas de BI. <br>
Mais informações em: https://www.hitachivantara.com/go/pentaho.html.<br>
2.5.2 Microsoft Power BI<br>
Com o crescimento de negócios das empresas, uma grande massa de dados que entram nessas empresas também aumentou e por causa disso, ficou difícil organizar, analisar, monitorar e compartilhar essa quantidade de informações. Para resolver esse problema, foi necessário criar ferramentas que pudessem atender a esses requisitos. <br>
Dentre várias ferramentas que foram criadas, têm-se o Power BI. É um pacote de ferramentas de análise de negócios que tem como objetivo a visualização, organização e análise de dados. O Power BI é uma ferramenta baseada na nuvem, tornando possível, ao usuário, a conexão de seus dados em tempo real, ou seja, em qualquer lugar que eles estejam, com rapidez, eficiência e compreensão. Além disso, ele permite a criação de painéis de simples visualização, fornecimento de relatórios interativos e o compartilhamento desses dados obtidos. <br>
Sendo uma ferramenta de Business Intelligence, o Power BI não é apenas para a inserção de dados e criação de relatórios. Ela transforma os dados brutos em informações significativas e úteis a fim de analisar o negócio, permite uma fácil interpretação do grande volume de dados e entrega análises que ajudam na decisão de uma grande variedade de negócios, variando do operacional ao estratégico. Também é capaz de limpar os dados formatados de forma irregular, garantindo que tenham apenas aqueles interessados ao negócio e modela os dados quem vêm de diversas fontes para que se consiga fazer com que esses dados trabalhem de forma eficiente. <br>
Mais informações em: https://powerbi.microsoft.com/pt-br/.<br>
<br>
<br>
<br>
3 ESTUDO DE CASO: MEC E INEP<br>
	Nessa seção serão explicados os trabalhos semelhantes a este e uma explicação sobre o MEC e o INEP.<br>
3.1 Demandas e observações sobre os dados do INEP<br>
Outras pesquisas semelhantes já foram feitas nessa área de estudo, tais como o trabalho apresentado por Oliveira (2018) quando apresenta alguns aspectos históricos e contemporâneos do negro e discorre de maneira abrangente sua atuação no sistema educacional brasileiro, trazendo aspectos históricos desde as épocas da Colônia, Império e Primeira República até os dias atuais. Já no artigo de Almeida e Sanchez (2016) é apresentada uma compreensão das legislações que regem a vida do negro na sua caminhada educacional para a visibilidade e valorização deles na construção do cotidiano escolar, passando pelo início da entrada do negro na educação formal brasileira até a atuação do movimento negro nessas práxis. <br>
No artigo de Oliveira (2013) a autora procura discorrer sobre a atuação da lei 10.639 para os professores, diante do contexto da educação básica e da questão racial. Zandona (2008) discorre sobre a problemática da desigualdade racial dos negros no contexto do ensino médio brasileiro, abordando temas como o racismo e a questão socioeconômica para o entendimento desse fato. <br>
Passos (2010) discorre sobre a população negra e as dificuldades enfrentadas por ela no contexto da Educação de Jovens e Adultos (EJA), passando pela construção dessas desigualdades e pelas políticas nacionais aplicadas para a promoção da igualdade. O texto de Fonseca et al. (2001) faz uma compilação de diversos artigos voltados para a atuação do negro sob várias perspectivas, como a educação das crianças negras no contexto da promulgação da Lei do Ventre Livre, de 1871; análise de projetos e iniciativas sobre as relações raciais voltadas as escolas da rede municipal de Belo Horizonte; análise do perfil dos estudantes negros ingressantes nos cursos de Direito; análise das questões de raça e gênero das graduandas negras da Unicamp. <br>
No ano de 2015 foi divulgado pelo MEC – Ministério da Educação, um relatório com o “Título de Educação para Todos”, nele foi feito um estudo centrado em vários aspectos da educação. Segundo a própria pesquisa, foram resumidos em seis principais tópicos, são eles: Cuidados e Educação na Primeira Infância, Educação Primária Universal, Habilidades de Jovens e Adultos, Alfabetização de Adultos, Paridade e Igualdade de Gênero, Qualidade da Educação. <br>
Em virtude disso, este trabalho de conclusão traz a contribuição de uma análise com uma alta especificação, focada no panorama da atuação do aluno negro voltada ao recorte temporal de 2015 a 2018 no contexto da educação básica brasileira utilizando a base de micro dados do Censo Escolar do INEP.<br>
3.2 MEC e INEP<br>
A história do Ministério da Educação é antiga, começando em 1930 quando foi criado no governo de Getúlio Vargas, inicialmente era chamado de Ministério dos Negócios da Educação e Saúde Pública. Como se pode ver pelo nome, a educação não era o único foco de atividade. Apenas em 1995, no governo de Fernando Henrique Cardoso, a educação ficou exclusiva ao ministério. A sigla MEC surgiu em 1953, quando se criou o Ministério da Educação e Cultura.<br>
Até 1960 de acordo com o MEC (2015), ele era centralizado, um modelo que era seguido por todos os municípios e estados. A primeira Lei de Diretrizes e Bases da Educação (LDB), que fora aprovada em 1961, diminuiu a centralização do MEC, fazendo assim os órgãos municipais e estaduais ganharem mais autonomia.<br>
Em 2015 a primeira versão da BNCC (Base Nacional Comum Curricular) é disponibilizada, uma pauta muito debatida e vista como importante para a educação. Somente em 2017 ela foi homologada para Ensino Básico e um ano depois, para o Ensino Médio.<br>
A Base é um documento normativo da maior importância, porque define <span class='f1 c_2' id='c_2_1' href='#c_2_2' cs_f='.c_2' >o conjunto de</span> aprendizagens essenciais que todos os alunos devem desenvolver ao longo da Educação Básica e do Ensino Médio, e orientar as propostas pedagógicas de todas as escolas públicas e privadas de Educação Infantil, Ensino Fundamental e Ensino Médio, em todo o Brasil (MEC, 2015).<br>
O Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (INEP) é vinculado ao MEC e engloba várias áreas educacionais, desde ensino básico até a graduação. A respeito de sua origem, é preciso considerar que:<br>
Chamado inicialmente de Instituto Nacional de Pedagogia, o Inep foi criado, por lei, em 13 <span class='f1 c_5' id='c_5_1' href='#c_5_2' cs_f='.c_5' >de janeiro de</span> 1937, no Rio de Janeiro. Foi em 1938, entretanto, que o órgão iniciou, de fato, seus trabalhos. A publicação do Decreto-Lei nº 580 regulamentou a organização e a estrutura da instituição, além de modificar sua denominação para Instituto Nacional de Estudos Pedagógicos. [...] em 1952, assumiu a direção do Instituto o professor Anísio Teixeira, que passou a dar maior ênfase ao trabalho de pesquisa. Seu objetivo era estabelecer centros de pesquisa como um meio de fundar em bases científicas a reconstrução educacional do Brasil. A ideia foi concretizada com a criação do Centro Brasileiro de Pesquisas Educacionais (CBPE), com sede no Rio de Janeiro, e dos Centros Regionais, nas cidades de Recife, Salvador, Belo Horizonte, São Paulo e Porto Alegre. Tanto o CBPE como os Centros Regionais estavam vinculados à nova estrutura do Inep (INEP, 2015).<br>
	Na década de 70, com a sede sendo transferida para Brasília e o CBPE sendo extinto, fez com que o modelo que fora idealizado por Anísio Teixeira fosse finalizado, que causou um reconhecimento ao INEP tanto nacionalmente quanto internacionalmente. Nos anos 80, passou por uma reforma institucional após um período de dificuldades, e tinha dois objetivos, que eram reorientação das políticas de apoio a pesquisas educacionais e o reforço do processo de disseminação de informações educacionais.<br>
	O INEP que conhecido hoje é devido à incorporação do Serviço de Estatística da Educação e Cultura (SEEC) à Secretaria de Avaliação e Informação Educacional (SEDIAE) que de acordo com o INEP (2015) a partir de 1997 um único órgão encarregado das avaliações, pesquisas e levantamentos estatísticos educacionais no âmbito do governo federal passou a existir através de uma integração do SEDIAE ao INEP. Nesse mesmo ano, o INEP foi transformado em autarquia federal.<br>
3.3 Como os dados são coletados e disponibilizados<br>
Os dados oferecidos pelo MEC, INEP e outros órgãos do governo são dados abertos, o que significa que estão disponíveis para todos usarem e também redistribuírem como quiserem, sem restrição de patentes, licenças ou parecido. Cada órgão disponibiliza os seus dados de acordo com seu Plano de Dados Abertos (PDA) e é responsável pela catalogação do mesmo.<br>
No caso do INEP eles podem ser acessados no seu próprio site, no link: http://inep.gov.br/web/guest/microdados, onde haverá uma página nominada “Micro dados”, lá estão separados por categoria e ano. Além disso, para outros dados, tem-se o Portal Brasileiro de Dados Abertos que possui os dados do INEP e de diversos outros órgãos, no link: http://dados.gov.br/.<br>
De acordo com o Portal Brasileiro de Dados Abertos (2017) em 2007 um grupo de 30 pessoas se reuniu na Califórnia - EUA, para definir os princípios dos Dados Abertos Governamentais. Eles criaram oito princípios, que são:<br>
Completos: Todos os dados públicos são disponibilizados, que no caso, não são sujeitos a limitações válidas de privacidade, segurança ou controle de acesso, reguladas por estatutos.<br>
Primários: Os dados são publicados do mesmo jeito que fora coletada na fonte, e não de forma agregada ou transformada.<br>
Atuais: Os dados são disponibilizados o mais rápido possível para preservar o seu valor.<br>
Acessíveis: Os dados são públicos para o maior público possível.<br>
Processáveis por máquina: Os dados são estruturados para possibilitar o seu processamento automático.<br>
Acesso não discriminatório: Os dados estão disponíveis para todos sem necessidade de identificação.<br>
Formatos não proprietários: Os dados estão disponíveis sem que nenhum ente tenha exclusivamente um controle.<br>
Licenças livres: Os dados não estão sujeitos a restrições como dito no parágrafo acima.<br>
<br>
<br>
<br>
4 DESCRIÇÃO DA MONTAGEM DO AMBIENTE<br>
	Nessa seção serão descritos todos os passos, técnicas e dados utilizados para a aplicação da metodologia de Business Intelligence no contexto do presente trabalho.<br>
4.1 Introdução<br>
	Segundo Braghittoni (2017, p. 1): “O BI é <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >um conjunto de</span> conceitos e métodos para melhorar o processo de tomada de decisão, utilizando-se de sistemas fundamentados em fatos e dimensões”. Nesse caso pode-se perceber que o BI é uma metodologia, que possui regras, ordem e práticas para sua aplicação. Sendo assim, é necessário descrever cada uma das partes que vão compor o ambiente de inteligência, utilizando como base os autores Braghittoni (2017), Carvalhaes e Alves (2015), Inmon (2005) e Kimball e Ross (2013).<br>
	Esse ambiente divide-se em (Figura 2):<br>
Parte 1: Fontes de Dados (Data Source), em que é feita a definição da localização dos dados, seu formato e quais deles serão aproveitados para a análise.<br>
Parte 2: Área de Staging (Staging Area), passo intermediário e opcional onde os dados são gravados, na sua forma original, em tabelas para posterior transformação e inserção.<br>
Parte 3: Data Warehouse (DW), que adquire os dados do banco Staging, após serem feitas as transformações para que eles possam ser utilizados pela ferramenta de BI. Sua criação pode ser antes ou depois de um Data Mart.<br>
Parte 4: Data Marts, que são pequenos DW relacionados a um assunto específico. Sua criação pode ser antes ou depois de um Data Warehouse dependendo a abordagem escolhida. Tais abordagens serão explicadas adiante.<br>
Parte 5: Análise dos resultados, em que a ferramenta de BI escolhida acessa os dados de um Data Warehouse ou de um Data Mart para que sejam feitas as gerações das análises.<br>
Suas definições serão explicadas a frente.<br>
Figura 2 - Arquitetura do ambiente de BI<br>
<br>
Fonte: Panoly (2019).<br>
4.2 Montagem do ambiente – Fontes de Dados (Data Source).<br>
	O primeiro passo na aplicação dos processos de Business Intelligence é definir quais serão as bases de dados utilizadas para o processo e quais dados serão extraídos delas. No caso do presente trabalho, foram utilizadas as bases de micro dados do censo escolar do INEP, disponíveis no Portal Brasileiro de Dados Abertos no link: http://dados.gov.br/dataset/microdados-do-censo-escolar e no próprio site do INEP no link: http://inep.gov.br/web/guest/microdados. Para a melhor delimitação do trabalho, foram utilizados os censos dos anos de 2015 a 2018. <br>
	Os arquivos estão em formato CSV (Comma-separated Values) que <span class='f1 c_3' id='c_3_1' href='#c_3_2' cs_f='.c_3' >é um tipo de</span> arquivo onde seus dados estão separados por algum delimitador, no caso das bases do INEP é utilizado o delimitador Pipe (|). Eles são divididos em Turmas, Escolas, Matriculas (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), e Docentes (Centro-Oeste, Nordeste, Norte, Sudeste e Sul), onde se encontra as informações das turmas, das escolas, dos alunos e dos docentes envolvidos nos censos de cada ano, respectivamente.<br>
	Além dos dados principais, faz-se necessário o uso de tabelas auxiliares para auxiliar na definição dos dados do INEP, já que são utilizados campos com os códigos dos Países, Unidades da Federação (UF), Municípios, Distritos, Mesorregiões e Microrregiões. Para o primeiro, o INEP disponibiliza em sua base, ao fazer download, uma tabela (Figura 3) que contêm os códigos dos países descritos no censo, já que alunos estrangeiros também são envolvidos no censo escolar.<br>
Figura 3 - Tabela de códigos dos países<br>
<br>
Fonte: Adaptado de INEP (2019).<br>
Para as UF, Municípios, Distritos, Mesorregiões e Microrregiões, foram utilizadas as bases de códigos Geodata disponíveis no site GitHub no link: https://github.com/paulofreitas/geodata-br/tree/master/data/pt. O GitHub é um site para a criação de repositórios públicos e privados com o intuito de compartilhar informações e códigos e o repositório Geodata tem como propósito prover informações precisas e atualizadas acerca dos dados geográficos do Brasil. Essas informações são um compilado das informações disponíveis na SIDRA (Sistema IBGE de Recuperação Automática) formados pelo IBGE (Instituto Brasileiro de Geografia e Estatística).<br>
4.3 Montagem do ambiente – Área de Staging<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a não volatilidade, ou seja, os dados dentro do mesmo não podem sofrer alterações. Isso significa que se faz necessária uma fase intermediária antes de carregar os dados no DW, para isso tem-se a Staging Area ou Data Stage. Com todos os dados já na máquina é iniciada a montagem dos processos de ETL para fazer a carga no Banco de Dados de Staging.<br>
	Será utilizado o Pentaho Data Integrator (PDI) versão 5.0.1 para iniciar os processos de ETL, separando as cargas por assunto. O PDI utiliza duas nomenclaturas como Job e Transformation, o primeiro é a menor ação possível que o programa possa fazer como ler o arquivo ou fazer inserção, e o segundo é <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >um conjunto de</span> outros Jobs para fazer uma execução única e contínua. Como na imagem abaixo:<br>
Figura 4 - Exemplo de Transformation e Job<br>
<br>
Fonte: Pentaho (20-?).<br>
	A carga dos arquivos no BD dos arquivos principais (turmas, matrícula, escolas, docentes) e das bases de códigos das UF, Municípios, Distritos, Mesorregiões e Microrregiões são compostas por três passos, em que o PDI encontra os arquivos, prepara-os para a inserção e grava-os no BD, como pode ser visto na imagem abaixo:<br>
Figura 5 - Visão da ETL das bases principais<br>
<br>
Fonte: Autores (2019).<br>
	Os passos são descritos abaixo:<br>
	Get File Names: Esse step procura nomes de arquivos ou pastas. Ele é recomendado para quando se tem uma grande massa de dados em que todos precisam ser gravados. Os padrões dos nomes são adquiridos conforme uma expressão regular.<br>
	Text File Input: Aqui o Pentaho prepara um ou mais arquivos de textos para a inserção, nele são configuradas diversas opções como os delimitadores do texto, linha de título, formato e colunas adicionais para serem adicionadas no momento da carga.<br>
	Table Output: Realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Já para a carga das tabelas contendo o código dos países, foi utilizado um padrão de carga diferente, já que o arquivo que possui esse dado está em um formato diferente das outras bases, como pode ser visto na imagem abaixo:<br>
Figura 6 - Visão geral da ETL de auxiliares<br>
<br>
Fonte: Autores (2019).<br>
Os passos são descritos abaixo:<br>
	Microsoft Excel Input: Esse step procura nomes de arquivos do tipo XLS (formato utilizado nas versões de 97 até 2003) e/ou XLSX (utilizado na versão de 2007 em diante). Nele podem-se configurar opções como, especificar de qual linha e/ou coluna deve-se iniciar a análise, se os títulos das colunas estão na primeira linha (Header), além de especificar campos adicionais no momento da carga.<br>
	Table Output: Como descrito nas cargas principais, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Após definir cada uma das ETLs, será usado uma Transformation para unir todos os outros Jobs, como pode ser visto na imagem abaixo:<br>
Figura 7 - Visão geral da ETL Staging<br>
<br>
Fonte: Autores (2019).<br>
Com todo o fluxo executado, o banco de dados Staging foi finalizado na forma da imagem abaixo:<br>
Figura 8 - Visão do Banco Staging<br>
<br>
Fonte: Autores (2019).<br>
4.4 Montagem do ambiente – Data Warehouse<br>
Após o banco Staging estar completamente carregado, será iniciado os processos para a formação do armazém de dados.<br>
4.4.1 Fato e Dimensão<br>
	Em uma modelagem multidimensional temos dois tipos de tabelas principais: Fato e Dimensão. <br>
Pela definição de Kimball (2013) dimensão é uma coleção de atributos semelhantes ao texto que estão altamente correlacionados entre si. Isso quer dizer que ela possui característica descritiva. Para se criar uma dimensão podem ser feitas algumas perguntas como “quando”, “onde”, “quem”, e “o que”. Já na tabela fato, normalmente os dados são apenas números, categorizando-a em quantitativa, mas também se podem ter textos que estão classificando o fato em análise. <br>
4.4.2 Abordagem Inmon x Kimball<br>
Antes de começar o desenvolvimento das ETLs, deve-se pensar como será a estrutura e modelo do Data Warehouse, tendo como base a abordagem Inmon ou Kimball. Vale ressaltar que não há uma escolha certa ou errada, mas aquela que atende melhor os requisitos e necessidades da organização.<br>
	Inmon utiliza a abordagem top-down em que o DW é um repositório de dados centralizado, sendo assim o componente mais importante da organização (PANOLY, 2019). Ele é o primeiro modelo criado logo após a extração de dados, e após sua finalização são criados todos os Data Marts necessários. Seu diagrama é mostrado abaixo:<br>
Figura 9 - Modelo Inmon<br>
Fonte: Panoly (2019).<br>
	Em contrapartida, Kimball utiliza a abordagem bottom-up em que é feita primeiramente a criação de Data Marts em cada área de interesse para depois se criar um grande Data Warehouse que é unicamente uma junção de todos esses Marts (PANOLY, 2019). Tal como Kimball (2013) afirma: “O Data Warehouse não é nada mais do que uma junção de diversos Data Marts”. Seu diagrama é mostrado abaixo:<br>
Figura 10 - Modelo Kimball<br>
Fonte: Panoly (2019).<br>
	Abaixo é feita uma comparação entre as abordagens Inmon e Kimball:<br>
Tabela 1 - Comparação entre as abordagens do Inmon e Kimball<br>
Fonte: Autores (2019)<br>
	Para a realização desse trabalho foi escolhida a abordagem Inmon porque o projeto não terá uso de Data Marts, assim sendo, será criado unicamente o Data Warehouse para armazenar os dados.<br>
4.4.3 Modelos Estrela e Floco de Neve (Star Schema and Snow-Flake Schema)<br>
	Tendo definida a estrutura, inicia-se o desenvolvimento do modelo do DW. <br>
Em um modelo de dados multidimensional pode ser utilizado dois tipos de modelos, que são: tipo Estrela (Star Schema) ou tipo Floco de Neve (Snow-Flake Schema).<br>
O modelo Estrela é o mais básico e mais comum para a arquitetura do Data Warehouse. No seu desenho, a tabela fato (F_VENDA, Figura 11) assume o centro da arquitetura seguido pelas tabelas de dimensões, que em volta dela, definem a quantidade de pontas da Estrela (CARVALHAES e ALVES, 2015). Possui como vantagem uma visualização simplificada dos dados, além de mais agilidade nas análises.<br>
Figura 11 - Exemplo de modelo Estrela<br>
Fonte: Autores (2019).<br>
O modelo Floco de Neve é um modelo específico que, partindo do modelo Estrela, as dimensões que possuem hierarquia são decompostas em outras tabelas (CARVALHAES e ALVES, 2015). Nesse modelo tem-se uma redução de redundância nas tabelas de dimensões e uma menor quantidade de memória utilizada. Um exemplo seria a dimensão chamada Data, em que ela poderá ser decomposta em outras tabelas como dia, mês, ano, trimestre, etc. Assim, essas “sub-dimensões” vão compor a dimensão principal. Seu diagrama é mostrado abaixo:<br>
Figura 12 - Exemplo de modelo Floco de Neve<br>
<br>
Fonte: Autores (2019).<br>
	Abaixo é feita uma comparação entre os modelos Estrela e Floco de Neve:<br>
Tabela 2 - Comparação entre Star Schema e Snow Flake Schema<br>
Fonte: Autores (2019).<br>
Para o presente trabalho, será utilizado o modelo Floco de Neve. Devido algumas dimensões apresentar hierarquia nelas, houve-se a necessidade de criar uma tabela adicional.<br>
4.4.4 Indicadores levantados para as análises<br>
	Ao desenvolver uma plataforma de BI, o objetivo é sempre responder a perguntas utilizando dados, que por sua vez se transformam em informação e auxílio na tomada de decisão. Para isso é necessário levantar perguntas que serão os indicadores da análise, com isso, serão definidas as fatos e dimensões. As perguntas levantadas pelo grupo são as seguintes:<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Qual é a diferença de alunos negros entre as regiões Nordeste e Sudeste nos anos da análise?<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Qual a quantidade de alunos negros nos módulos de ensino Presencial, Semipresencial e a Distância entre os anos da análise?<br>
Qual a quantidade de alunos negros que moram em zona Urbana ou Rural entre os anos da análise?<br>
Qual a quantidade de alunos negros que estudam em escolas Públicas e Privadas?<br>
Qual a quantidade de alunos negros que estudam em escolas Urbanas e Rurais?<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Com as perguntas concluídas, pode-se agora levantar os fatos e dimensões da análise. Será utilizada apenas uma tabela fato, que é a tabela de matrículas (alunos) e as seguintes dimensões: Tempo (Ano), Localidade Município (Município, UF, País), Localidade Distrito (Microrregião, Município, UF, Região, Mesorregião, Distrito) e Escola.<br>
Com a fato e as dimensões já definidas, será criada as ETLs para a carga das informações no Data Warehouse.<br>
4.4.5 Processo ETL para carga do Data Warehouse<br>
	Nessa parte de explicação das ETLs, será separado por dimensões que possuem padrões de carga semelhantes, explicando os dados envolvidos e o processo.<br>
4.4.5.1 Definição dos indicadores nulos<br>
	Segundo Braghittoni (2017, p. 94) nenhuma coluna que esteja inserida no Data Warehouse pode aceitar valores nulos (null). O site W3Schools (2019) define valores null como um campo que não possui valor, deixado em branco no momento da gravação. Sendo assim, há a necessidade de criar valores genéricos para definir um valor que veio nulo. Em cada uma das dimensões explicadas adiante, será descrito os seus respectivos indicadores nulos.	<br>
4.4.5.2 Dimensão Tempo (Ano)<br>
	Inmon (2005, p. 29) define em um dos seus postulados sobre Data Warehouse a variabilidade com o tempo, ou seja, um DW e suas informações vivem com base no tempo. Com base dessa informação, Braghittoni (2017, p. 31) atesta que “Por mais que não exista nenhuma outra dimensão no seu DW, a dimensão temporal deve estar lá” e também (2017, p. 73) “Como postulado por Inmon, o DW é sempre variável com o tempo, ou seja, a dimensão DATA deve invariavelmente existir”.<br>
	Conforme definido pelos dois autores, todo projeto necessita obrigatoriamente de uma dimensão de tempo, em contrapartida, esse ‘tempo’ pode ser descrito de formas diferentes por cada projeto, com base nas necessidades das análises. Ele pode ser definido tanto como informações separadas (ano ou mês ou dia), uma data, formada por ano, mês, e dia, ou até uma informação mais complexa inserindo trimestre, dia da semana, hora, etc.<br>
	Para o presente trabalho, a dimensão de tempo será identificada pelos anos referentes a cada análise (2015 até 2018), um identificador para cada uma delas e seu indicador nulo que será explicado adiante.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 13 - Visão geral da ETL Ano<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Data Grid: Neste step será criada uma tabela <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >com um conjunto</span> constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada um dos anos da análise e um indicador para cada um deles.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada. Aqui os dados estão sendo gravados na tabela D_TEMPO do banco de dados.<br>
	Indicador nulo da dimensão:<br>
	Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3 Dimensões Localidade<br>
	Como descrito na seção 4.4.4 acerca dos indicadores e das dimensões, será usado duas tabelas chamadas de Aluno e Matrícula, elas por sua vez utilizam informações geográficas na sua estrutura. <br>
Em base dos micro dados do INEP, as tabelas sobre Aluno utilizam as informações de município, UF, e país, por outro lado, a dimensão Escola faz uso das informações de distrito, município, UF, microrregião, mesorregião e região.<br>
Tendo em vista que cada uma das tabelas apresenta uma combinação de informações diferentes, fez-se necessário dividir as informações de localidade, com cada uma sendo chamada pelo seu menor grão de informação. No caso das combinações de Aluno, a dimensão com as suas combinações será chamada de Localidade Município, e de Escola, chamada de Localidade Distrito, por este ser o menor nível de informação. Essas informações geográficas seguem a seguinte ordem (do maior para o menor):<br>
País;<br>
Região;<br>
UF;<br>
Mesorregião;<br>
Microrregião;<br>
Município;<br>
Distrito;<br>
As definições de cada uma das dimensões Localidade serão explicadas adiante.<br>
4.4.5.3.1 Dimensão Localidade Distrito<br>
	Como explicado anteriormente, uma das tabelas será a Localidade Distrito que irá apoiar as combinações da tabela Escola. Essa tabela de Localidade é formada pela combinação das informações de distrito, município, microrregião, mesorregião, UF e região, junto de um identificador único para cada uma dessas combinações, além dos identificadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 14 - Diagrama da ETL Localidade Distrito<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: distrito, municipio, microrregiao, mesorregiao, uf): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >por meio de</span> um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Data Grid: Neste step será criada uma tabela <span class='f1 c_1' id='c_1_1' href='#c_1_2' cs_f='.c_1' >com um conjunto</span> constante de dados, informando os nomes dos campos, seus tipos, e seus respectivos dados. Aqui está sendo carregado cada uma das regiões da análise.<br>
Sort rows (na imagem acima com os nomes: Sort distrito, Sort municipio, Sort microrregiao, Sort mesorregião, Sort regiao, Sort distrito_municipio, Sort municipio_microrregiao, Sort microrregiao_mesorregiao, Sort mesorregiao_uf): Esse passo possibilita a ordenação de <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >um conjunto de</span> dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge distrito_municipio, Merge municipio_microrregiao, Merge microrregiao_mesorregiao, Merge mesorregiao_uf, Merge uf_regiao): Com um funcionamento semelhante ao comando JOIN do SQL, esse step une dois fluxos de informação com base em uma coluna compartilhada (key), <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >além de ser</span> possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados da tabela (step Table input com o nome ‘distritos’) que contêm as informações dos códigos de distritos (menor nível de informação, daí o nome da dimensão), dados estes inseridos previamente na seção 4.3 de montagem do Staging. Além desses códigos, são lidos também os códigos referentes aos municípios associados a cada distrito, na própria tabela de distritos e na tabela de municípios (step Table input com o nome ‘municipio’). <br>
	<span class='f1 c_13' id='c_13_1' href='#c_13_2' cs_f='.c_13' >No momento que</span> todas as informações são adquiridas, o próximo passo é ordená-las (step Sort rows com os nomes ‘Sort distrito’ e ‘Sort municipio’) de modo ascendente escolhendo a coluna que contêm os códigos dos municípios, esse passo de ordenação é necessário para o funcionamento correto do próximo passo.<br>
	Após as informações ordenadas, é feita a ‘união’ desses dois fluxos de informação (step Merge Join com o nome ‘Merge distrito_municipio’) utilizando como coluna de união os códigos de município em cada um dos fluxos. Por exemplo: na tabela de distritos, um distrito de nome Lua Nova (cód. 521295610) possui nas suas informações o código de município 5212956, fazendo a união desse código na tabela de municípios, é encontrado esse código associado ao nome do município Matrinchã. Esse processo é feito para todos os distritos na tabela distritos no banco Staging.<br>
	A próxima tabela a ser acessada é a que contêm as informações dos códigos das Microrregiões (step Table input com o nome ‘microrregiao’). Como descrito no processo da tabela distrito, essa tabela foi carregada na seção 4.3 do banco de Staging. Em conjunto desses, no momento da carga da tabela anterior de municípios também foi adquirida as informações referentes aos códigos Microrregiões associadas a cada uma.<br>
	Tal como no processo anterior, após as informações serem adquiridas, elas são ordenadas (step Sort rows com os nomes ‘Sort microrregiao’ e ‘Sort distrito_municipio’) de modo ascendente, agora utilizando a coluna com os códigos das Microrregiões como referência.<br>
	Após isso é feita a ‘união’ (step Merge Rows com o nome ‘Merge município_microrregiao’) desses fluxos tal como o processo anterior, mas utilizando a coluna com o código das Microrregiões como forma de união. Por exemplo: continuando com o município anteriormente especificado (Matrinchã, cód. 5212956), ele possui em sua base o código de Microrregião 52002, que na tabela de Microrregião o código está associado ao nome Rio Vermelho.<br>
	O mesmo processo é aplicado a seguir, com as informações de Mesorregião sendo adquiridas (step Table input com o nome ‘mesorregiao’) e ordenadas (step Sort rows com os nomes ‘Sort mesorregiao’ e ‘Sort municipio_microrregiao’) pelo seu respectivo código junto com as informações de mesorregião adquiridas na tabela de microrregião, sendo feita a sua união (step Merge Rows com o nome ‘Merge microrregiao_ mesorregiao’) no final do processo.<br>
	Repetindo os processos anteriores, adquirem-se as informações dos códigos das UFs brasileiras (step Table input com o nome ‘uf’) e é feita a sua ordenação junto com o resultado da união anterior (step Sort rows com os nomes ‘Sort uf’ e ‘Sort microrregiao_mesorregiao’) e posteriormente a sua união (step Merge Rows com o nome ‘Merge mesorregião_uf’) com base nas informações dos códigos das UFs na ordenação anterior.<br>
	Por último, são adquiridas as informações sobre as regiões brasileiras (step Data Grid com o nome ‘regiao’), feita sua ordenação e da união anterior (step Sort rows com os nomes ‘Sort regiao’ e ‘Sort mesorregiao_uf’) e a união desses resultados com base na coluna de código das regiões (step Merge Rows com o nome ‘Merge uf_regiao’).<br>
	Finalmente, após todos os resultados serem retornados é usado o step Select values para remover as colunas redundantes geradas no momento das uniões, mantendo uma coluna para cada código e seus respectivos nomes, sendo ordenado logo após (step Sort rows) e inserido na sua dimensão de localidade (step Table output).<br>
	Indicadores nulos da dimensão:<br>
Como explicado no início da seção, serão atribuídos valores para serem inseridos para caso o campo da informação, no momento da carga, for nulo. Para essa dimensão será usado o indicador ‘-1’ para caso em algum momento da carga essa informação estiver nula.<br>
4.4.5.3.2 Dimensão Localidade Município<br>
	Para a segunda tabela, tem-se a Localidade Município. A tabela em questão vai apoiar as combinações da fato Aluno, composta por município, UF e país. Além do identificador único para cada combinação e indicadores nulos.<br>
	Seu diagrama de carga é mostrado na imagem abaixo:<br>
Figura 15 - Diagrama da ETL Localidade Município<br>
<br>
Fonte: Autores (2019).<br>
Os seguintes passos foram utilizados:<br>
Table Input (na imagem acima com os nomes: municipio, uf, pais): Como descrito na carga anterior, este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >por meio de</span> um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. <br>
Sort rows (na imagem acima com os nomes: Sort municipio, Sort uf, Sort pais, Sort municipio_uf, Sort pais_uf, Sort cartesian): Como dito na carga anterior, esse passo possibilita a ordenação de <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >um conjunto de</span> dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas. <br>
Merge Join (na imagem acima com os nomes: Merge município_uf, Merge pais_uf): Explicado na carga anterior, possui um funcionamento semelhante ao comando JOIN do SQL. Esse step une dois fluxos de informação com base em uma coluna compartilhada (key), <span class='f1 c_6' id='c_6_1' href='#c_6_2' cs_f='.c_6' >além de ser</span> possível a configuração da forma de união (retornar apenas os dados que se relacionam ou também aqueles que não se relacionam). Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida. <br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Execute SQL Script: Conforme explicado anteriormente, nesse step o Pentaho permite executar um ou mais comandos SQL para fazer alguma operação no BD, seja uma consulta (SELECT) ou uma inserção (INSERT). Além disso, é possível utilizar variáveis criadas no próprio PDI no código. <br>
Add constants: Neste passo é criado um fluxo constante de dados para serem inseridos junto com outro fluxo. Nele é possível configurar o nome da coluna que vai gerar esses dados, bem como os próprios dados a serem gerados. <br>
Join rows (cartesian product): Tem o funcionamento parecido com o Merge Join, mas no seu caso, ele é usado para multiplicar dois fluxos de informações criando todas as combinações possíveis entre eles, fazendo o chamado ‘produto cartesiano’. <br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. <br>
	Table Output: Conforme explicado na parte do Staging, esse step realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	O uso de todos os passos descritos anteriormente serão explicados adiante.<br>
	Descrição do processo de carga:<br>
	Este processo de carga inicia-se com a aquisição dos dados dos códigos e nomes da tabela de Municípios (step Table Input com o nome ‘municipio’) carregadas na seção 4.3 de Staging junto com os códigos das UFs associadas a eles, além dos dados dos códigos e nomes das UFs brasileiras (step Table Input com o nome ‘uf’). Junto com essa carga, é adicionado o código ‘76’ que é referente ao Brasil para ser carregado junto (step Add constants). <br>
Ao final dessas duas cargas, são feitas suas respectivas ordenações (step Sort rows com os nomes ‘Sort municipio’ e ‘Sort uf’). Após suas ordenações concluídas, é feita a união dos dois fluxos de informações (step Merge Join com o nome ‘Merge município_uf’) utilizando como coluna de união os códigos das UFs.<br>
Junto da carga anterior, é feita a aquisição dos dados referentes aos códigos dos países (step Table Input com o nome ‘pais’) e sua ordenação ascendente pelo mesmo (step Sort rows com o nome ‘Sort pais’). Com os dois steps de ordenação prontos (‘Sort pais’ e ‘Sort município_uf’) é feita a cópia de seus dados para cada um dos passos seguintes: o primeiro (step Merge Join com o nome ‘Merge pais_uf’), faz a união dos dados dos municípios e UFs com o código ‘76’ que é referente ao país Brasil. O segundo (step Join Rows (cartesian product)), faz todas as combinações possíveis dos dados provenientes de municípios e UFs com os outros países, isso foi feito para ter as combinações dos alunos nascidos no exterior/naturalizados, que nasceram em outro país, mas que residem no Brasil. Ao final, os dois fluxos são mais uma vez ordenados (step Sort rows com o nome ‘Sort pais_uf’ e ‘Sort cartesian’).<br>
Após a finalização das ordenações anteriores, são removidas as colunas redundantes resultantes das uniões (step Select values) e checados os seus valores nulos (step If field is null), que nessa situação, serão os alunos estrangeiros que, na base do INEP, não possuem registros de UF/Município de nascimento/endereço, diferente dos alunos nascidos no exterior/naturalizados que possuem um país estrangeiro e registro de UF/Município de nascimento. Ao final é feita sua ordenação ascendente pelo código do país (step Sort rows) e sua inserção na respectiva dimensão no Data Warehouse (step Table output).<br>
Como passo independente, ou seja, <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >que pode ser</span> executado antes ou depois da inserção dos dados no DW, tem-se a inserção dos indicadores nulos (step Execute SQL script) na dimensão de combinações de municípios. Esses indicadores serão detalhados adiante.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso todas as informações estiverem nulas. Esse indicador foi criado em duas combinações: Quando a informação de município, UF e país de origem for nula (-1, -1, -1); quando o aluno tem como país de origem o Brasil, mas não possui informações referentes ao seu município e sua UF (-1, -1, 76).<br>
-2: Caso o aluno for estrangeiro. Esse indicador foi criado em uma combinação: Quando o aluno tiver como país de origem qualquer valor diferente de 76 (que faz referência ao Brasil) e suas informações de UF e município não estiverem disponíveis, por exemplo, se o aluno for natural dos Estados Unidos: (-2, -2, 840).<br>
-3: Caso o aluno for naturalizado/nascido no exterior. Esse indicador foi criado em uma combinação: Quando o aluno for naturalizado/nascido no exterior e suas informações de município e UF não estiverem disponíveis (-3, -3, 76).<br>
4.4.5.4 Dimensão Escola<br>
Para a carga da dimensão das escolas, a ordem de ações consiste na extração dos dados dos dois tipos de escolas (públicas e privadas), transformação dos dados inserindo os indicadores de nulo dos dois tipos de escolas, remover as informações duplicadas e posterior inserção delas no Data Warehouse, como segue:<br>
Figura 16 - Diagrama da ETL Escola<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input: Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >por meio de</span> um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de dois passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus dois usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os dois fluxos de dados para centralizar a inserção.<br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >um conjunto de</span> dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
Unique rows: Esse step é utilizado para eliminar dados que porventura venham duplicados no fluxo de carga, além disso, podem ser configurados contadores para a quantidade de duplicatas encontradas e redirecionamento delas para outro passo. Requer o uso do step Sort rows antes deste para ordenação da coluna escolhida.<br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >que pode ser</span> renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >por meio de</span> uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas as aquisições dos dados referentes às escolas em duas partes, o primeiro (step Table input) procura as escolas públicas e o segundo (step Table input 2) procura as escolas privadas conforme a coluna TP_DEPENDENCIA de cada um deles. Caso a escola tiver os códigos 1, 2, 3 ela <span class='f1 c_12' id='c_12_1' href='#c_12_2' cs_f='.c_12' >é considerada uma</span> escola pública por ter dependência nas esferas federal, estadual ou municipal, respectivamente, ou ela possui o código 4 referente a uma escola privada.<br>
	Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos substituem os valores nulos por dois tipos de indicadores. Na pesquisa de escolas públicas, é feita a substituição dos dados (step If Field Value is Null) acerca dos mantedores de escolas privadas e da categoria privada da mesma, já que por ser uma escola pública, esses indicadores não se aplicam a ela e sempre estarão nulos, além da substituição quando essa informação estiver vazia. No outro passo onde é feita a pesquisa de escolas privadas, é feita a substituição de todos os valores nulos (step If Field Value is Null 2). Os indicadores nulos dessa dimensão serão explicados adiante.<br>
	Com as substituições concluídas, o Dummy é utilizado como o step que recebe todo esse fluxo de dados para centralizá-los e enviar para o próximo step em que é feita a sua ordenação (step Sort rows), e após ser ordenado, é feita remoção das escolas duplicadas (step Unique rows) para manter uma lista única com todas as escolas envolvidas na análise.<br>
	Tendo os dados duplicados removidos, é feita a procura (step Database lookup) do código referente a cada combinação de região, distrito, microrregião, mesorregião, UF e município na tabela D_LOCALIDADE_DISTRITO, carregada anteriormente para apoiar essas combinações. Quando uma combinação é encontrada com sucesso, é retornado para o fluxo o código referente a essa combinação.<br>
	Com todas as combinações encontradas na dimensão de localidade distrito, é feita a substituição dos dados (step Replace in string) de algumas colunas com informações referentes às escolas pelo o seu significado segundo o dicionário de dados do INEP. Por exemplo, a coluna IN_AGUA_INEXISTENTE indica se a escola possui ou não abastecimento de água, no fluxo, os dados existentes nessa coluna são 0 para ‘Não’ e 1 para ‘Sim’, assim, esse step faz essa substituição do valor numérico pelo seu valor de significado. Ao finalizar, os dados são mais uma vez ordenados e inseridos na tabela da dimensão escolar.<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente, essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula. <br>
-2: Inserido nas colunas referentes aos mantedores privados e na categoria de escola privada, quando a escola for pública, já que essas duas informações não se aplicam a uma escola que é pública.<br>
4.4.5.5 Fato Aluno<br>
	Agora na última tabela do Data Warehouse, tem-se a fato aluno que é gerada após todas as dimensões estiverem prontas no BD.<br>
	Na sua carga, o processo é semelhante à carga da dimensão escolas, com o diferencial da substituição dos anos da análise por seus respectivos códigos definidos na dimensão ano no momento da carga.<br>
Figura 17 - Diagrama da ETL Aluno<br>
<br>
Fonte: Autores (2019).<br>
Os passos estão descritos a seguir:<br>
Table Input (na imagem acima com os nomes: Table input brasileiros, Table input naturalizados, Table input estrangeiros): Este step permite utilizar os dados já existentes em alguma tabela para fazer outras operações, como a inserção em outro banco, por exemplo. Os dados nesse passo são adquiridos <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >por meio de</span> um comando SQL, mas o Pentaho possui uma interface gráfica para selecionar esses dados, sem necessidade do comando, se assim o usuário preferir. A utilização de três passos será explicada adiante.<br>
If Field Value is Null: Aqui nesse passo o Pentaho permite inserir valores em campos que estão nulos. É possível escolher um valor padrão ou especificar valores para cada uma das colunas que chegam ao fluxo de dados. Seus três usos nessa ETL serão explicados adiante.<br>
Dummy (do nothing): Esse step não realiza ações, mas pode ser utilizado para unir diferentes fluxos de dados ou analisar os dados que estão sendo recebidos. Nessa ETL, ele está juntando os três fluxos de dados para centralizar a inserção.<br>
Replace In String: Esse passo permite que o PDI possa substituir um valor de algum campo por outro valor especificado. Esse campo pode ser especificado diretamente ou <span class='f1 c_11' id='c_11_1' href='#c_11_2' cs_f='.c_11' >por meio de</span> uma expressão regular, após, é informado o valor de procura e depois o novo valor, que também pode estar em outro campo. <br>
Database lookup: Esse passo permite a comparação de um ou mais valores vindos de um fluxo de dados com uma ou mais colunas inseridas em uma tabela no banco de dados. Essa comparação, quando verdadeira, retorna uma coluna específica no banco, <span class='f1 c_7' id='c_7_1' href='#c_7_2' cs_f='.c_7' >que pode ser</span> renomeado. Além disso, podem ser feitas configurações para o uso de cache, que em dadas situações podem melhorar a performance da carga. Seu uso nessa ETL será explicado adiante.<br>
Select values: Como explicado na carga anterior, este passo é utilizado para remover colunas, alterar o nome delas bem como seus tipos. <br>
Sort rows: Como dito na carga anterior, esse passo possibilita a ordenação de <span class='f1 c_8' id='c_8_1' href='#c_8_2' cs_f='.c_8' >um conjunto de</span> dados com base em uma coluna informada. Seu uso é semelhante ao comando order by do SQL. Nele podem ser configuradas outras opções como ordenação ascendente ou descendente e diferenciação de maiúsculas e minúsculas.<br>
	Table Output: Conforme explicado na parte do Staging, esse passo realiza a carga dos dados estruturados em uma tabela no banco de dados. A tabela não precisa ser criada com antecedência, já que o PDI prepara um comando SQL automaticamente para a mesma ser criada.<br>
	Descrição do processo de carga:<br>
	Primeiramente, são feitas três pesquisas de dados. A primeira (step Table input com o nome ‘Table input brasileiros’) procura os estudantes brasileiros, a segunda (step Table input com o nome ‘Table input naturalizados’) procura os estudantes naturalizados, a terceira (step Table input com o nome ‘Table input estrangeiros’) procura os estudantes estrangeiros, conforme a coluna CO_PAIS_ORIGEM de cada um deles. Caso o aluno possuir o código 76 nessa coluna, significa que ele tem nacionalidade brasileira/naturalizado (esse é o código do Brasil na tabela de países do INEP). Caso contrário, ele é definido como estrangeiro.<br>
Após a pesquisa dos dados tem-se a parte de transformação, em que os próximos passos (step If field value is null) substituem os valores nulos por até três tipos de indicadores, esses indicadores serão explicados adiante. <br>
	Com as substituições concluídas, é feita a junção dos três fluxos de dados (step Dummy (do nothing)) e envio para o próximo passo (step Replace in string) que faz a substituição dos dados relativos aos anos da análise (2015, 2016, 2017 e 2018) pelos seus códigos definidos na tabela dimensão ano (1, 2, 3 e 4, respectivamente), além de indicadores referentes ao sexo do aluno, cor/raça, nacionalidade, entre outros. <br>
	Ao ser feitas as substituições, são feitas as comparações das combinações de município, UF e país de origem (tanto como nascimento e endereço), com o seu equivalente na dimensão D_LOCALIDADE_MUNICIPIO (step Database lookup e Database lookup 2), essa combinação ao ser verdadeira, retorna o código associado a ela existente na dimensão. <br>
Concluindo esse passo, é feita a remoção das colunas com os códigos das UFs, municípios e país de origem (tanto como nascimento e endereço) para manter apenas o código referente à combinação dessas três informações para que possa ser ligada a dimensão D_LOCALIDADE_MUNICIPIO, após é feita a sua ordenação e finalmente a carga dessas informações na tabela fato dos alunos (step Table output).<br>
Indicadores nulos da dimensão:<br>
Como citado anteriormente no step If field value is null essa dimensão possuirá os seguintes indicadores de informação nula:<br>
-1: Caso alguma informação estiver nula.<br>
-2: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é estrangeiro.<br>
-3: Indicador usado para representar uma informação nula nas colunas de UF e município de um aluno que é naturalizado.<br>
Nessa última carga são finalizadas todas as dimensões e a fato referente ao ambiente de BI do presente trabalho, com o banco de dados pronto para o próximo passo, a montagem das análises pela ferramenta escolhida.<br>
<br>
<br>
<br>
5 RESULTADOS DA ANÁLISE<br>
	Ao finalizar todas as ETLs para o Data Warehouse e geração dos indicadores pela ferramenta Power BI foi possível realizar a análise desses indicadores. Foram gerados quinze indicadores apresentados abaixo, a partir da leitura, interpretação de como eles poderiam ser úteis como indicadores e da bibliografia consultada constante no capítulo 3 deste trabalho. <br>
Este trabalho não tem a pretensão de cobrir todo o assunto, mas pode ser útil para indicar caminhos para outras análises. É importante lembrar que essas análises são afetas a um recorte temporal, a saber, os anos de 2015 a 2018 da Educação Básica dos estados, municípios e distritos brasileiros.<br>
Qual o total de alunos por cada Cor/Raça definida pelo Censo Escolar entre os anos da análise?<br>
Figura 18 - Contagem de cor/raça por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico da figura acima a cor/raça de maior quantidade da base é dos alunos que se consideraram pardos, mantendo quase 20 milhões de alunos entre todos os anos da análise, logo após tem-se a Cor/Raça branca, atingindo quase 17 milhões, além dos que preferiram não se declarar, com a sua menor quantidade em 2018 onde estiveram com 14 milhões. Os negros se mantêm entre 1,8 e 1,9 milhões, sendo a quarta maior Cor/Raça na base do INEP. <br>
Qual o total de alunos que se declararam negros entre os anos da análise?<br>
Figura 19 - Contagem de alunos negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico para o segundo indicador, a quantidade de alunos negros na base do INEP não passou de 1,9 milhões. Sua menor quantidade foi em 2018 onde se teve apenas 1,8 milhões de alunos que se declararam negros e seu maior pico foi em 2017 tendo 1,87 alunos com auto declaração negra.<br>
Qual o total de alunos estrangeiros que se declararam negros entre os anos da análise?<br>
Figura 20 - Contagem de alunos estrangeiros negros por ano<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico referente ao terceiro indicador, é possível notar um aumento desde o início da análise em 2015, de 3,7 mil alunos, até o último ano onde chegou a marca de quase 10 mil alunos estrangeiros segundo a base do INEP.<br>
Qual o país que possui a maior quantidade de alunos estrangeiros negros no Brasil entre os anos da análise?<br>
Figura 21 - Contagem de alunos estrangeiros negros por país (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
	Para o quarto indicador, por questões de visualização, a análise foi reduzida para mostrar apenas os dez primeiros países que mais possuem alunos no Brasil, na educação básica segundo a base do INEP. O Haiti se mostra o país com a maior quantidade, chegando a quase 15 mil alunos, seguido por Angola e Congo. Portugal é o quarto país e os Estados Unidos estão abaixo desses 10 primeiros.<br>
	Não faz parte deste trabalho a correlação das missões de paz no Haiti com o fato deste país ser aquele com mais estrangeiros negros na Educação Básica, porém trata-se de um possível estudo futuro.<br>
Qual a UF que possui a maior concentração de alunos estrangeiros negros?<br>
Figura 22 - Contagem de alunos estrangeiros negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Para o gráfico acima, por questões de visualização, o gráfico foi reduzido para mostrar apenas os 10 primeiros. A UF onde mais se concentram os alunos estrangeiros negros é o estado de São Paulo, seguido por Santa Catarina e Paraná. O Distrito Federal é o nono maior, chegando a quase mil registros.<br>
Qual o total de alunos negros por região, UF e município entre os anos da análise?<br>
Figura 23 - Contagem de alunos negros por região<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima, o maior registro de alunos se concentra na região sudeste onde tem-se 3,59 milhões de dados, seguido logo após pelo nordeste com 2,34 milhões de registros, o sul vem depois com 65 mil resultados, após o norte com 40 mil resultados, e por último o centro-oeste com 33 mil resultados.<br>
Figura 24 - Contagem de alunos negros por UF (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), a maior quantidade de registros dos alunos negros se concentra no estado da Bahia, com 1,3 milhões de resultados. Logo após vem São Paulo com 1,24 milhões e Minas Gerais com 1,14 milhões, fechando os três primeiros. O Distrito Federal não fica entre os 10 primeiros nessa análise.<br>
Figura 25 - Contagem de alunos negros por município (Top 10)<br>
 <br>
Fonte: Autores (2019).<br>
	Segundo o gráfico acima (que por questões de visualização, foi reduzido para mostrar apenas os 10 primeiros), o município com a maior concentração de alunos negros é o município de Salvador na Bahia com 440 mil alunos, seguido pelo município do Rio de Janeiro com 415 mil resultados e após o município de São Paulo com 363 mil resultados. Brasília aparece na análise como o sexto maior município com 83 mil alunos (na base do INEP, Brasília, mesmo sendo oficialmente um distrito, possui um código de município para manter a padronização).<br>
Qual é a diferença de alunos negros entre as regiões nordeste e sudeste nos anos da análise?<br>
A figura 23 do indicador anterior também responde este, demonstrando a diferença de quase 1,25 milhões entre a região sudeste e nordeste.<br>
Qual é a quantidade de alunos negros no Distrito Federal entre os anos da análise?<br>
Figura 26 - Contagem de alunos negros no DF por ano<br>
<br>
Fonte: Autores (2019).<br>
Em análise ao gráfico, é possível observar que, em média, cerca de 25.675 estudantes negros estão anualmente matriculados em escolas de ensino básico. Segundo dados da Secretária de Estado da Educação do DF, cerca de 450 mil estudantes são atendidos pela Secretária de Educação do Distrito Federal, envolvendo Escolas de Educação Básica, Escolas Parque, Centros Interescolares de Línguas, Centros de Ensino Profissionalizante, além de um Centro de Ensino Médio Integrado (SECRETARIA DE ESTADO DA EDUCAÇÃO, 2018). Em dados levantados pela Codeplan (2014), a população negra do DF corresponde a 57%, porém esta realidade não é transmitida no gráfico, pois a avaliação é feita por auto declaração, sendo vários alunos não se declarando como negros ou nem sequer declaram uma cor/raça.<br>
Qual a quantidade de alunos negros que estudam em escolas sem água, energia, esgoto e alimentação entre os anos da análise?<br>
Figura 27 - Contagem de alunos negros com água inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
	Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
A partir de uma análise do gráfico, é possível identificar que uma parcela de 17,8 mil alunos negros no ano de 2015 não tem acesso a água em suas escolas. Esta quantidade pode estar ligada a fatores geográficos e geopolíticos, pois segundo uma pesquisa realizada pelo jornal O Globo, casos como este, onde há falta de um dos princípios de saneamento básico, são extremados, afetando pincipalmente comunidades pobres e rurais (VASCONCELLOS, RIBEIRO e LINS, 2014). Também consta no gráfico a quantidade de dados inexistentes na base do INEP (representado por “-1”), onde em 2017 teve-se a maior quantidade de dados faltantes, 6,4 mil.<br>
Figura 28 - Contagem de alunos negros com energia inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Com base no gráfico, é possível detectar que a quantidade de 2,9 mil alunos negros no ano de 2015 que frequentam escolas com déficit de energia elétrica. Este número pode ser considerado baixo em relação à quantidade total de alunos negros, pois a partir de programas de desenvolvimento como o Programa Luz para Todos que asseguram a prioridade de desenvolvimento de medidas para o melhoramento do acesso à energia em escolas (PLANO DE DESENVOLVIMENTO DA EDUCAÇÃO, [200-?]). As informações inexistentes passam as informações existentes em todos os anos, chegando ao seu maior pico em 2017 com 6,4 mil de registros.<br>
Figura 29 - Contagem de alunos negros com alimentação inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, tem-se a informação que em média, 1,6 milhões de alunos negros estudam em escolas sem alimentação, tendo o seu maior pico em 2017, onde teve-se 1,71 milhões de registros. As informações inexistentes não chegam a quase mil registros, se mostrando a menor inexistência entre os outros gráficos.<br>
Figura 30 - Contagem de alunos negros com esgoto inexistente nas escolas por ano<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida.<br>
Segundo o gráfico acima, em média, 12,7 mil alunos estudam em escolas sem esgoto, com o seu maior pico em 2015 com quase 14 mil registros de alunos. Sobre as informações inexistentes, sua maior quantidade foi em 2017, onde teve-se 6,4 mil registros faltantes.<br>
Qual a quantidade de alunos negros por sexo entre os anos da análise?<br>
Figura 31 - Contagem de alunos por sexo por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se uma maior quantidade de alunos negros envolvidos na educação básica, segundo os dados da base do INEP. O maior pico foi em 2017, onde teve-se 981 mil alunos, contra 885 mil alunas no mesmo ano. Percebe-se também que nos dois sexos apresentados, essa quantidade foi variando, onde em 2015 teve-se uma quantidade maior que em 2016, que teve uma quantidade menor que em 2017, que teve uma quantidade maior em 2018.<br>
 Qual a quantidade de alunos negros nos módulos de ensino presencial, semipresencial e a distância entre os anos da análise?<br>
Figura 32 - Contagem de alunos negros por mediação pedagógica por ano<br>
<br>
Fonte: Autores (2019).<br>
	Segundo o gráfico apresentado, percebe-se a maior inserção dos negros na educação presencial com quase 100% dos dados da base demonstrando isso.<br>
Qual a quantidade de alunos negros que moram em zona urbana ou rural entre os anos da análise?<br>
Figura 33 - Contagem de alunos negros por zona residencial por ano<br>
<br>
Fonte: Autores (2019).<br>
Partindo de uma análise do gráfico, pode-se observar que grande parte da população de alunos negros se concentra em áreas urbanas, sendo mais exatos 83,96% desta amostra, e apenas 16,04% da população negra concentra-se em zonas rurais. Isso pode ser por conta de uma tendência nacional de concentração de população urbana. Segundo uma pesquisa do IBGE, cerca de 84,72% da população brasileira está reunida em centros urbanos e apenas 15,28% está localizada em zonas rurais (IBGE, 2015).<br>
Qual a quantidade de alunos negros que estudam em escolas públicas e privadas?<br>
Figura 34 - Contagem de alunos negros por dependência escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo o gráfico há uma maior concentração de alunos negros em escolas com dependência municipal, chegando até 52,72% no ano de 2015. Logo após tem-se a modalidade estadual, com o seu pico atingindo 70 mil registros no ano de 2017. As escolas privadas não passaram de 21 mil registros em todos os anos da análise.<br>
Qual a quantidade de alunos negros que estudam em escolas urbanas e rurais?<br>
Figura 35 - Contagem de alunos negros por localização escolar por ano<br>
<br>
Fonte: Autores (2019).<br>
Segundo os dados apresentados no gráfico, percebe-se uma maior quantidade de alunos negros envolvidos nas escolas de localização urbana, tendo seu maior pico em 2017, onde tem-se 1,67 milhões de alunos nas escolas urbanas, comparado com o maior pico das escolas rurais em 2015, com 20 mil alunos.<br>
Qual a quantidade de alunos negros em cada etapa de ensino definida no censo entre os anos da análise?<br>
Por questões de performance, não foram alterados os códigos referentes a cada uma das etapas de ensino, mas em cada um dos gráficos será descrito o significado dos mesmos segundo o dicionário de dados na base do INEP.<br>
Figura 36 - Contagem de alunos negros por etapa de ensino em 2015 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
-1: Informação desconhecida;<br>
14: Ensino Fundamental de 9 anos - 1º Ano;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 226 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 136 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 124 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 3º Ano com 111 mil registros.<br>
Figura 37 - Contagem de alunos negros por etapa de ensino em 2016 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
15: Ensino Fundamental de 9 anos - 2º Ano;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 144 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 140 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 125 mil, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano com 111 mil registros.<br>
Figura 38 - Contagem de alunos negros por etapas de ensino em 2017 (Top 10)<br>
<br>
Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é o indicador de informação desconhecida, ou seja, na base, por algum motivo, essa informação estava vazia, tendo nela 200 mil registros. Tirando o indicador nulo, o próximo dado de maior quantidade é o de Educação Infantil – Pré-escola com 141 mil registros. Logo após, a etapa de Ensino Fundamental de 9 anos - 6º Ano com 120 mil registros, e finalizando os três primeiros, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 111 mil registros.<br>
Figura 39 - Contagem de alunos negros por etapa de ensino em 2018 (Top 10)<br>
<br>
	Fonte: Autores (2019).<br>
Significado dos códigos da análise:<br>
	1: Educação Infantil – Creche;<br>
-1: Informação desconhecida;<br>
16: Ensino Fundamental de 9 anos - 3º Ano;<br>
17: Ensino Fundamental de 9 anos - 4º Ano;<br>
18: Ensino Fundamental de 9 anos - 5º Ano;<br>
	19: Ensino Fundamental de 9 anos - 6º Ano;<br>
2: Educação Infantil - Pré-escola;<br>
20: Ensino Fundamental de 9 anos - 7º Ano;<br>
21: Ensino Fundamental de 9 anos - 8º Ano;<br>
25: Ensino Médio - 1ª Série;<br>
Segundo o gráfico acima (por questões de visualização foi reduzido para mostrar apenas os 10 primeiros), o dado em primeiro lugar da análise é a etapa Educação Infantil - Pré-escola, com 141 mil registros. O indicador nulo, diferentemente dos gráficos anteriores desse indicador, vem em segundo lugar com 129 mil registros. Tirando o indicador nulo, tem-se a etapa Ensino Fundamental de 9 anos - 6º Ano, com 118 mil. Logo após, a etapa Ensino Fundamental de 9 anos - 7º Ano e a etapa Ensino Médio - 1ª Série com 108 mil registros. Por último, fechando os três primeiros (desconsiderando o indicador nulo), tem-se a etapa Ensino Fundamental de 9 anos - 8º Ano com 99 mil registros.<br>
<br>
<br>
<br>
<br>
6 CONSIDERAÇÕES FINAIS<br>
	Este estudo possibilitou uma análise do panorama da atuação do aluno negro na educação básica brasileira demonstrando a utilidade e todo o processo de Business Intelligence.<br>
Foram analisados quase 200 milhões de alunos envolvidos na base do INEP segundo o recorte temporal de 2015 a 2018, o que, em média seria 50 milhões de alunos para cada um dos anos da análise. Para os alunos negros, tem-se, em média, 1,8 milhões de registros em cada um dos anos, finalizando um total de 7 milhões de alunos negros.<br>
	De maneira geral, o Business Intelligence disponibiliza aos usuários formas claras de ver os dados, a partir de visualizações gráficas limpas e bem estruturadas. Desde o início teve-se o foco em demonstrar e implementar uma aplicação de BI tradicional por inteira. A implantação de um projeto de BI não é simples e neste caso não foi diferente, havendo uma longa fase de planejamento para a correta estruturação e carga dos dados.<br>
	Uma das maiores dificuldades no decorrer do trabalho foi a performance da carga dos dados, onde a cada erro constatado, era necessária a completa limpeza no Data Warehouse, procurar em qual parte da carga teve-se o erro e finalmente realizar a nova carga, custando muito ao computador. No começo, teve-se muitos problemas de falta de memória por causa das cargas feitas, esse problema foi contornado pelo uso das cargas incrementais, onde as cargas eram separadas por ano ou por região, isso facilitou, inclusive, a correção de erros que as cargas poderiam apresentar.<br>
	Este estudo nos ofereceu a oportunidade para que integrar todos os conhecimentos adquiridos no curso de Ciências da Computação, tais como a vivência das matérias de Lógica de Programação, Bancos de Dados e Tópicos de Atuação Profissional. O uso as ferramentas de BI foram essenciais para o fechamento do processo.<br>
	Por fim, com este estudo é possível entender o processo de Business Intelligence, bem como funcionalidades e características; os processos de ETL com o uso de uma ferramenta Open Source, o Pentaho; comparações entre as duas abordagens e modelos utilizados pelos autores da área. Mas como tudo que envolve tecnologia, tende a evoluir, novas tecnologias para o BI surgirão. Também é possível, a partir deste estudo, contextualizar a atuação do aluno negro na educação básica brasileira de maneira geral nos anos de 2015 a 2018 com o auxílio das análises.<br>
6.1 Limitações e Trabalhos Futuros<br>
	O trabalho possui certas limitações que abrem espaço para melhorias e trabalhos futuros, como o desenvolvimento de uma página web/software mobile ou desktop para a visualização dessas análises; implementação de um processo de Machine Learning/Deep Learning para a previsão, conforme os dados da base, de quantos alunos negros a base pode receber nos próximos anos; desenvolvimento de Data Marts para o foco das análises em mais indicadores; expandir o ambiente para unir os dados de todos os censos. O trabalho não cobre nenhum desses itens citados anteriormente, e como dito, abre espaço para uma grande melhoria.<br>
6.2 Revisão dos objetivos alcançados<br>
	Sobre os objetivos específicos citados na seção 1.4.2, todos eles foram alcançados com sucesso, onde: <br>
Foi possível levantar o estado da arte no que tange Business Intelligence, sua metodologia e processos no capítulo 2. <br>
Foram apresentados indicadores sobre a atuação do aluno negro na educação brasileira que podem ser úteis em futuros trabalhos para essa área na seção 4.4.4. <br>
Foi possível aplicar a metodologia de Business Intelligence, os processos de ETL e a montagem do ambiente de Data Warehouse no capítulo 4, onde foi criado todo um ambiente de Business Intelligence que possibilitou as análises. <br>
Foram desenvolvidos os resultados das análises através da ferramenta de BI escolhida, onde foram gerados gráficos e visualizações dos dados.<br>
<br>
<br>
<br>
<br>
Inmon	Kimball<br>
Manutenção	Fácil	Difícil - muitas vezes redundante e sujeito a revisão<br>
Tempo	Maior tempo para iniciar	Menor tempo para iniciar<br>
Conhecimento preciso	Time especialista	Time generalista<br>
Tempo de construção do Data Warehouse	Demorado	Rápido<br>
Custo para implantar	Custos iniciais altos, com menores custos subsequentes de desenvolvimento do projeto	Custos iniciais pequenos, com cada projeto subsequente custando-o mais<br>
Persistência dos dados	Alta taxa de mudança dos dados	Relativamente estável<br>
<br>
Star Schema	Snow Flake<br>
Manutenção	Possui dados redundantes que dificultam manter ou alterar	Sem redundância, portanto, facilita manter e alterar<br>
Facilidade de Uso	Consultas menos complexas e de fácil entendimento	As consultas são mais complexas o que torna difícil de entender<br>
Performance nas consultas	Menos foreign keys o que torna a consulta mais rápida	Mais foreign keys o que torna a consulta mais lenta<br>
Espaço	Não tem tabelas normalizadas o que aumenta o espaço	Tem tabelas normalizadas<br>
Bom para:	Bom para Data Mart com relacionamentos simples (1:1 ou 1:N)	Bom para uso em Data Warehouse para simplificar as relações complexas (N:N</div>
<!-- DIVISOR_DIV_CANDIDATE -->

</div><script   src="https://code.jquery.com/jquery-3.1.0.min.js"   integrity="sha256-cCueBR6CsyA4/9szpPfrX3s49M9vUU5BgtiJj06wt/s="   crossorigin="anonymous"></script><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.6/cerulean/bootstrap.min.css" rel="stylesheet" integrity="sha384-fUMURLTdEcpeYHly3PAwggI3l2UvdHNg/I+8LRph7hLDcAZm77YfDx3Tjum9d3vK" crossorigin="anonymous">